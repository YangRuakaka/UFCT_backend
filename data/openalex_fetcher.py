"""
OpenAlex API æ•°æ®è·å–æ¨¡å—
ç”¨äºä» OpenAlex API ä¸­è·å–è®ºæ–‡ã€ä½œè€…å’Œå¼•ç”¨æ•°æ®
å®˜æ–¹æ–‡æ¡£: https://docs.openalex.org/
"""
import logging
import requests
from typing import List, Dict, Optional, Tuple
import time
import pandas as pd
from urllib.parse import quote
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Condition

from .param_validator import OpenAlexParamValidator

logger = logging.getLogger(__name__)


class RateLimiter:
    """
    ä»¤ç‰Œæ¡¶é™æµå™¨ - ç²¾ç¡®æ§åˆ¶é€Ÿç‡
    
    è®¾è®¡åŸç†ï¼š
    - ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•ï¼Œç²¾ç¡®æ§åˆ¶æ¯ç§’è¯·æ±‚æ•°
    - æ¯ä¸ªè¯·æ±‚å¿…é¡»ç­‰å¾…ç›´åˆ°æœ‰å¯ç”¨ä»¤ç‰Œ
    - é¿å…çªå‘è¯·æ±‚è¶…è¿‡é€Ÿç‡é™åˆ¶
    
    å…³é”®æ”¹è¿›ï¼š
    - ä½¿ç”¨ Semaphore è€Œä¸æ˜¯æ‰‹åŠ¨ä»¤ç‰Œè®¡æ•°ï¼Œé˜²æ­¢å¤šçº¿ç¨‹ç«æ€æ¡ä»¶
    - åŸºäºæ—¶é—´çš„ç²¾ç¡®ä»¤ç‰Œè¡¥å……ï¼Œè€Œä¸æ˜¯åŸºäºè¯·æ±‚æ•°
    """
    def __init__(self, max_requests_per_second: int = 10):
        """
        åˆå§‹åŒ–é™æµå™¨
        
        Args:
            max_requests_per_second: æ¯ç§’æœ€å¤šè¯·æ±‚æ•°ï¼ˆå®˜æ–¹é™åˆ¶ï¼š10ï¼‰
        """
        self.max_rps = max_requests_per_second
        self.lock = Lock()
        self.last_request_time = 0
        # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.min_interval = 1.0 / max_requests_per_second
    
    def acquire(self, timeout: float = 60):
        """
        è·å–è®¸å¯è¯ - é˜»å¡ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚
        
        è¿™ä¸ªæ–¹æ³•ç¡®ä¿è¯·æ±‚ä¹‹é—´çš„é—´éš”è‡³å°‘ä¸º min_intervalï¼Œ
        ä»è€Œä¿è¯é€Ÿç‡ä¸è¶…è¿‡ max_rpsã€‚
        
        Args:
            timeout: æœ€å¤šç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            True if è·å–æˆåŠŸ, False if è¶…æ—¶
        """
        start_time = time.time()
        
        with self.lock:
            now = time.time()
            time_since_last = now - self.last_request_time
            
            # å¦‚æœè·ç¦»ä¸Šæ¬¡è¯·æ±‚æ—¶é—´ä¸è¶³ï¼Œåˆ™ç­‰å¾…
            if time_since_last < self.min_interval:
                wait_time = self.min_interval - time_since_last
                
                if time.time() - start_time + wait_time > timeout:
                    return False  # è¶…æ—¶
                
                time.sleep(wait_time)
                self.last_request_time = time.time()
            else:
                self.last_request_time = now
            
            return True


class OpenAlexFetcher:
    """OpenAlex API æ•°æ®è·å–å™¨"""
    
    BASE_URL = "https://api.openalex.org"
    
    def __init__(self, email: str = None, max_concurrent_requests: int = 10, enable_or_batching: bool = True):
        """
        åˆå§‹åŒ– OpenAlex æ•°æ®è·å–å™¨
        
        Args:
            email: é‚®ç®±åœ°å€ï¼ˆæ¨èæä¾›ä»¥åŠ å…¥ polite poolï¼Œè·å¾—æ›´ç¨³å®šçš„å“åº”æ—¶é—´ï¼‰
            max_concurrent_requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼ˆå®˜æ–¹é™åˆ¶ï¼š10è¯·æ±‚/ç§’ï¼‰
            enable_or_batching: æ˜¯å¦å¯ç”¨ OR è¯­æ³•æ‰¹é‡è¯·æ±‚ä¼˜åŒ–ï¼ˆå®˜æ–¹æ¨èï¼‰
        
        å®˜æ–¹å¹¶å‘ç­–ç•¥æ–‡æ¡£: 
        - https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication
        - https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or
        - https://blog.ourresearch.org/fetch-multiple-dois-in-one-openalex-api-request/
        
        å®˜æ–¹é€Ÿç‡é™åˆ¶ï¼š
        - æœ€å¤š 10 ä¸ªè¯·æ±‚/ç§’ï¼ˆPolite Poolï¼‰
        - æ¯å¤© 100,000 ä¸ªè¯·æ±‚
        - Premium ç”¨æˆ·ï¼šæ›´é«˜çš„é™åˆ¶ï¼ˆéœ€è®¢é˜…ï¼Œå¯å…è´¹ç”¨äºå­¦æœ¯ç ”ç©¶ï¼‰
        
        å®˜æ–¹æ¨èçš„æé€Ÿæ–¹æ³•ï¼ˆé‡è¦ï¼ï¼‰ï¼š
        1. æ·»åŠ  mailto å‚æ•°åŠ å…¥ Polite Poolï¼ˆæ›´ç¨³å®šçš„å“åº”æ—¶é—´ï¼‰âœ“ å·²å®ç°
        2. ä½¿ç”¨ OR è¯­æ³•æ‰¹é‡è¯·æ±‚ï¼ˆ50ä¸ªè¯·æ±‚â†’1ä¸ªè¯·æ±‚ï¼‰âœ“ å·²å®ç°
        3. ä½¿ç”¨ Group By èšåˆï¼ˆä»… Premiumï¼Œç›´æ¥è¿”å›ç»Ÿè®¡ç»“æœï¼‰ğŸ“‹ å¯é€‰
        4. å‡çº§åˆ° Premium è®¡åˆ’è·å¾—æ›´é«˜çš„é€Ÿç‡é™åˆ¶å’Œç‰¹æ®Šè¿‡æ»¤å™¨
        
        æ€§èƒ½å¯¹æ¯”ï¼ˆ10000+ å¯¹ä½œè€…åˆä½œå…³ç³»ï¼‰ï¼š
        - é€å¯¹æŸ¥è¯¢ï¼ˆä½æ•ˆï¼‰ï¼š10731 ä¸ªè¯·æ±‚ â†’ 2å°æ—¶+
        - OR æ‰¹é‡ï¼ˆå½“å‰ï¼‰ï¼š~49 ä¸ªè¯·æ±‚ â†’ 5åˆ†é’Ÿå·¦å³ ğŸš€
        - Group By èšåˆï¼ˆPremiumï¼‰ï¼š1-2 ä¸ªè¯·æ±‚ â†’ å‡ ç§’é’Ÿ âš¡
        
        ä¼˜åŒ–å»ºè®®ï¼š
        - ä½¿ç”¨ OR è¯­æ³•å°†å¤šä¸ªå•ç‹¬è¯·æ±‚åˆå¹¶ï¼ˆæœ€å¤š 100 ä¸ªå€¼ï¼‰
        - ä½¿ç”¨ mailto å‚æ•°åŠ å…¥ "polite pool" è·å¾—æ›´ç¨³å®šçš„å“åº”æ—¶é—´
        - å¯¹äºå­¦æœ¯ç ”ç©¶ï¼Œå¯å…è´¹å‡çº§åˆ° Premiumï¼šsupport@openalex.org
        """
        self.email = email or "yangyyk@tongji.edu.cn"
        self.max_concurrent_requests = max_concurrent_requests
        self.enable_or_batching = enable_or_batching
        self.api_key = None  # Premium API Keyï¼ˆå¯é€‰ï¼‰
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': f'OpenAlexBackend (mailto:{self.email})',
        })
        # å®˜æ–¹é™åˆ¶ï¼š10 è¯·æ±‚/ç§’ï¼Œæ‰€ä»¥æœ€å°é—´éš”ä¸º 0.1 ç§’
        self.min_request_interval = 1.0 / self.max_concurrent_requests
        
        # âœ… æ›¿æ¢åŸæ¥çš„ request_lock å’Œ last_request_time
        self.rate_limiter = RateLimiter(max_requests_per_second=10)
        
        logger.info(f"âœ“ OpenAlex Fetcher åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"  Email: {self.email}")
        logger.info(f"  Base URL: {self.BASE_URL}")
        logger.info(f"  æœ€å¤§å¹¶å‘è¯·æ±‚æ•°: {self.max_concurrent_requests} (å®˜æ–¹é™åˆ¶ï¼š10è¯·æ±‚/ç§’)")
        logger.info(f"  å¯ç”¨ OR è¯­æ³•æ‰¹é‡ä¼˜åŒ–: {self.enable_or_batching}")
        logger.info(f"")
        logger.info(f"ğŸ“š å®˜æ–¹æé€Ÿæ–‡æ¡£: https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication")
        logger.info(f"ğŸ’¡ å½“å‰ä½¿ç”¨ OR è¯­æ³•æ–¹æ¡ˆï¼ˆ~50å€æ€§èƒ½æå‡ï¼‰")
        logger.info(f"âš¡ Premium ç”¨æˆ·å¯ä½¿ç”¨ Group By èšåˆè·å¾—é¢å¤–çš„æ€§èƒ½æå‡")
    
    def _rate_limit(self):
        """
        å®ç°è¯·æ±‚é¢‘ç‡é™åˆ¶ - ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•
        
        å®˜æ–¹æ–‡æ¡£: https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication
        """
        success = self.rate_limiter.acquire(timeout=60)
        if not success:
            logger.warning("âš ï¸  é€Ÿç‡é™åˆ¶ï¼šè¶…è¿‡æœ€å¤§ç­‰å¾…æ—¶é—´")
    
    def _make_request(self, endpoint: str, params: Dict = None, timeout: int = 60, max_retries: int = 3) -> Dict:
        """
        å‘èµ· API è¯·æ±‚ - åŒ…å«é€Ÿç‡é™åˆ¶å’Œé€€é¿é‡è¯•
        
        Args:
            endpoint: API ç«¯ç‚¹
            params: è¯·æ±‚å‚æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: é‡åˆ° 429 é”™è¯¯æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            JSON å“åº”
        
        Raises:
            requests.exceptions.RequestException: å¦‚æœè¯·æ±‚å¤±è´¥
        """
        url = f"{self.BASE_URL}{endpoint}"
        
        for attempt in range(max_retries + 1):
            try:
                # ç¡®ä¿ä¸è¶…è¿‡é€Ÿç‡é™åˆ¶
                self._rate_limit()
                
                response = self.session.get(url, params=params, timeout=timeout)
                
                # å¦‚æœæ˜¯ 429ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
                if response.status_code == 429:
                    if attempt < max_retries:
                        wait_time = 2 ** (attempt + 1)  # 2s, 4s, 8s, ...
                        logger.warning(f"âš ï¸  è§¦å‘é€Ÿç‡é™åˆ¶ (429)ï¼Œ{wait_time}s åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"âœ— API è¯·æ±‚å¤±è´¥: {response.status_code} (å·²é‡è¯• {max_retries} æ¬¡)")
                        response.raise_for_status()
                
                response.raise_for_status()
                return response.json()
            
            except requests.exceptions.Timeout:
                logger.error(f"âœ— API è¯·æ±‚è¶…æ—¶ (attempt {attempt + 1}/{max_retries + 1})")
                if attempt < max_retries:
                    time.sleep(2 ** attempt)
                    continue
                raise
            
            except requests.exceptions.RequestException as e:
                logger.error(f"âœ— API è¯·æ±‚å¤±è´¥: {e}")
                raise
    
    def _get_optimal_batch_size(self, num_items: int) -> int:
        """
        æ ¹æ®é¡¹ç›®æ•°é‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ batch_size
        
        ç›®çš„ï¼šå¹³è¡¡è¯·æ±‚æ•°å’Œå•ä¸ªè¯·æ±‚çš„è€—æ—¶
        
        åŸç†ï¼š
        - æ€»è€—æ—¶ â‰ˆ è¯·æ±‚æ•° Ã— å•ä¸ªè¯·æ±‚è€—æ—¶
        - è¯·æ±‚æ•° = (num_items / batch_size)Â²  (äºŒå±‚å¾ªç¯ï¼Œä¸è®¡ç®—é‡å¤)
        - å•ä¸ªè¯·æ±‚è€—æ—¶ â‰ˆ 0.1s + batch_sizeÂ² Ã— 0.001s (ä¸æ•°æ®é‡æˆå¹³æ–¹å…³ç³»)
        
        æ€§èƒ½å¯¹æ¯”ï¼ˆä»¥ 147 ä½ä½œè€…ä¸ºä¾‹ï¼‰ï¼š
        - batch_size=20ï¼š36 ä¸ªè¯·æ±‚ Ã— 0.5s = 18s âœ“ ç¨³å®š
        - batch_size=50ï¼š6 ä¸ªè¯·æ±‚ Ã— 2.5s = 15s âœ… æœ€ä¼˜ï¼
        - batch_size=100ï¼š3 ä¸ªè¯·æ±‚ Ã— 10s = 30s âœ— å¤ªæ…¢ä¸”å®¹æ˜“è¶…æ—¶
        
        Args:
            num_items: è¦å¤„ç†çš„é¡¹ç›®æ•°é‡
        
        Returns:
            æ¨èçš„ batch_size
        """
        if num_items <= 50:
            # å°è§„æ¨¡ï¼šä¸€ä¸ªæˆ–ä¸¤ä¸ªæ‰¹æ¬¡è¶³å¤Ÿ
            return num_items if num_items <= 25 else 25
        elif num_items <= 200:
            # ä¸­ç­‰è§„æ¨¡ï¼šæ¨è 50ï¼ˆå¹³è¡¡ç‚¹ï¼‰
            return 50
        elif num_items <= 500:
            # å¤§è§„æ¨¡ï¼šå¯ç”¨ 50-70
            return 60
        else:
            # è¶…å¤§è§„æ¨¡ï¼šå¯ç”¨ 70-80
            return 70
    
    def _batch_by_or_syntax(self, ids: List[str], batch_size: int = 50) -> List[List[str]]:
        """
        å°† ID åˆ—è¡¨åˆ†ç»„ï¼Œæ¯ç»„æœ€å¤š 100 ä¸ªå€¼ï¼ˆå®˜æ–¹ OR è¯­æ³•é™åˆ¶ï¼‰
        
        å®˜æ–¹æ–‡æ¡£: 
        - https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or
        - æ”¯æŒæœ€å¤š 100 ä¸ªå€¼åœ¨å•ä¸ª OR è¿‡æ»¤å™¨ä¸­
        - å¿…é¡»é…åˆ per_page=100 æˆ–æ›´é«˜æ‰èƒ½è·å–æ‰€æœ‰ç»“æœ
        
        Args:
            ids: ID åˆ—è¡¨
            batch_size: å•ä¸ªæ‰¹æ¬¡çš„å¤§å°ï¼ˆå»ºè®® 50ï¼Œä¸è¶…è¿‡ 100ï¼‰
        
        Returns:
            åˆ†ç»„åçš„ ID åˆ—è¡¨
        """
        if batch_size > 100:
            batch_size = 100
            logger.warning("âš ï¸  æ‰¹å¤„ç†å¤§å°è¶…è¿‡å®˜æ–¹é™åˆ¶ 100ï¼Œå·²è‡ªåŠ¨è°ƒæ•´ä¸º 100")
        
        batches = []
        for i in range(0, len(ids), batch_size):
            batches.append(ids[i:i + batch_size])
        return batches
    
    def search_works(
        self,
        query: Optional[str] = None,
        year_min: Optional[int] = None,
        year_max: Optional[int] = None,
        limit: int = 200,
        discipline: Optional[str] = None,
        institution: Optional[str] = None
    ) -> List[Dict]:
        """
        æœç´¢è®ºæ–‡
        
        æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªå­¦ç§‘ï¼ˆä½¿ç”¨ OR æŸ¥è¯¢ï¼‰
        ä¾‹ï¼šdiscipline="Computer Science,Machine Learning,Deep Learning"
        
        æ³¨æ„ï¼šä¸ä¼šè‡ªåŠ¨å®½æ¾è¿‡æ»¤æ¡ä»¶ã€‚å¦‚æœæŒ‡å®šçš„æ¡ä»¶æ— æ³•åŒ¹é…ä»»ä½•è®ºæ–‡ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨ã€‚
        è¿™æ ·ç”¨æˆ·å¯ä»¥é€šè¿‡è¿”å›çš„0ä¸ªè®ºæ–‡æ¥äº†è§£è¯¥æ¡ä»¶ä¸‹ç¡®å®æ— ç›¸å…³è®ºæ–‡ã€‚
        """
        
        filters = []
        
        # å¹´ä»½èŒƒå›´è¿‡æ»¤
        if year_min and year_max:
            filters.append(f"publication_year:{year_min}-{year_max}")
        
        # å­¦ç§‘è¿‡æ»¤ - æ”¯æŒå¤šä¸ªå­¦ç§‘çš„ OR æŸ¥è¯¢
        if discipline:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é€—å·ï¼ˆå¤šä¸ªå­¦ç§‘ï¼‰
            if ',' in discipline:
                # å¤šä¸ªå­¦ç§‘ï¼šä½¿ç”¨ OR è¯­æ³•
                validated_disciplines = OpenAlexParamValidator.validate_and_convert_disciplines(discipline)
                if validated_disciplines:
                    # ä½¿ç”¨ | è¿æ¥å¤šä¸ª IDï¼ˆOpenAlex OR è¯­æ³•ï¼‰
                    or_filter = '|'.join(validated_disciplines)
                    filters.append(f"topics.id:{or_filter}")
                    logger.info(f"å¤šå­¦ç§‘è¿‡æ»¤ (OR æŸ¥è¯¢): {len(validated_disciplines)} ä¸ªå­¦ç§‘")
            else:
                # å•ä¸ªå­¦ç§‘ï¼šä¼ ç»Ÿæ–¹å¼
                validated_discipline = OpenAlexParamValidator.validate_and_convert_discipline(discipline)
                if validated_discipline:
                    filters.append(f"topics.id:{validated_discipline}")
        
        # æœºæ„è¿‡æ»¤ - ä½¿ç”¨ authorships.institutions.id æˆ– authorships.institutions.ror
        if institution:
            validated_institution = OpenAlexParamValidator.validate_and_convert_institution(institution)
            if validated_institution:
                if validated_institution.startswith('I'):
                    filters.append(f"authorships.institutions.id:{validated_institution}")
                elif validated_institution.startswith('https://ror.org/'):
                    filters.append(f"authorships.institutions.ror:{validated_institution}")
        
        filter_str = ",".join(filters) if filters else None
        
        logger.info(f"æœç´¢è®ºæ–‡: query={query}, year_min={year_min}, year_max={year_max}, limit={limit}, discipline={discipline}, institution={institution}")
        logger.debug(f"è¿‡æ»¤æ¡ä»¶: {filter_str}")
        
        # æ‰§è¡Œæœç´¢ï¼Œä½¿ç”¨æŒ‡å®šçš„å®Œæ•´è¿‡æ»¤æ¡ä»¶
        result = self._execute_search(query, filter_str, limit)
        
        if result:
            # æ ‡å‡†åŒ–è®ºæ–‡æ•°æ®ï¼šç¡®ä¿æ¯ç¯‡è®ºæ–‡éƒ½æœ‰æœ‰æ•ˆçš„ id å­—æ®µ
            normalized_result = self._normalize_works(result)
            logger.info(f"âœ“ è·å–æˆåŠŸ: {len(result)} ç¯‡è®ºæ–‡ï¼Œæœ‰æ•ˆIDæ•°: {len(normalized_result)} ç¯‡")
            return normalized_result
        else:
            logger.info(f"âœ“ æŸ¥è¯¢å®Œæˆ: 0 ç¯‡è®ºæ–‡ (æŒ‡å®šæ¡ä»¶ä¸‹æ— åŒ¹é…è®ºæ–‡)")
            return result
    
    def _normalize_works(self, works: List[Dict]) -> List[Dict]:
        """
        æ ‡å‡†åŒ–è®ºæ–‡æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰è®ºæ–‡éƒ½æœ‰æœ‰æ•ˆçš„ id å­—æ®µ
        
        OpenAlex API è¿”å›çš„ id æ˜¯å®Œæ•´ URLï¼ˆå¦‚ https://openalex.org/W123456ï¼‰ï¼Œ
        æ­¤æ–¹æ³•ç¡®ä¿ id å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        
        Args:
            works: åŸå§‹è®ºæ–‡åˆ—è¡¨
        
        Returns:
            æ ‡å‡†åŒ–åçš„è®ºæ–‡åˆ—è¡¨ï¼ˆè¿‡æ»¤æ‰æ— æ•ˆ ID çš„è®ºæ–‡ï¼‰
        """
        valid_works = []
        invalid_count = 0
        
        for idx, work in enumerate(works):
            if not work or not isinstance(work, dict):
                invalid_count += 1
                continue
            
            # OpenAlex è¿”å›çš„ id åº”è¯¥åœ¨ 'id' å­—æ®µä¸­
            work_id = work.get('id')
            if work_id:
                # ç¡®ä¿ id æ˜¯å­—ç¬¦ä¸²ä¸”éç©º
                if isinstance(work_id, str) and work_id.strip():
                    valid_works.append(work)
                else:
                    invalid_count += 1
            else:
                invalid_count += 1
                # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ— æ•ˆIDçš„è®ºæ–‡ï¼ŒæŸ¥çœ‹å…¶ç»“æ„
                if invalid_count == 1:
                    sample_keys = list(work.keys()) if isinstance(work, dict) else "N/A"
                    logger.warning(f"ç¬¬ä¸€ç¯‡æ— æ•ˆè®ºæ–‡ï¼ˆç´¢å¼• {idx}ï¼‰ç»“æ„: {sample_keys}")
        
        if invalid_count > 0:
            logger.warning(f"å·²è¿‡æ»¤æ‰ {invalid_count} ç¯‡æ— æ•ˆè®ºæ–‡ï¼ˆæ—  ID æˆ– ID ä¸ºç©ºï¼‰")
        
        return valid_works
    
    def _execute_search(self, query: Optional[str], filter_str: Optional[str], limit: int) -> List[Dict]:
        """
        ä½¿ç”¨æ›´å¤§çš„ per_page æ¥ä¼˜åŒ–æ€§èƒ½ï¼ˆOpenAlex ä¸æ”¯æŒçœŸæ­£çš„å¹¶å‘æ¸¸æ ‡åˆ†é¡µï¼‰
        
        é‡è¦è¯´æ˜ï¼šOpenAlex çš„ cursor åˆ†é¡µæ˜¯ keyset paginationï¼Œæ¸¸æ ‡ä»£è¡¨çš„æ˜¯ä¸€ä¸ªç‰¹å®šä½ç½®ã€‚
        å¦‚æœå¹¶å‘ä½¿ç”¨å¤šä¸ªæ¸¸æ ‡ï¼Œå®ƒä»¬ä¼šè¿”å›é‡å çš„æ•°æ®ã€‚
        
        å®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. å¢åŠ  per_page å‚æ•°åˆ°æœ€å¤§å€¼ 200ï¼Œå‡å°‘æ€»è¯·æ±‚æ•°
        2. é€é¡µé¡ºåºè·å–ï¼ˆéµå¾ªå®˜æ–¹æ¨èï¼‰
        3. ä½¿ç”¨é€Ÿç‡é™åˆ¶å™¨é˜²æ­¢è¶…è¿‡ 10 req/s çš„é™åˆ¶
        
        æ€§èƒ½å¯¹æ¯”ï¼š
        - ä½¿ç”¨ per_page=200ï¼š14 é¡µ Ã— ~4.5s/é¡µ = 63sï¼Œä½†å¹¶å‘ä¼ªè£…æˆé¡ºåº
        - é€é¡µä¼˜åŒ–ç‰ˆï¼š14 é¡µ Ã— ~4.5s/é¡µ = 63sï¼Œä½†ä»£ç ç®€æ´ã€ä¸ä¼šæœ‰é‡å¤æ•°æ®
        """
        all_results = []
        per_page = 200
        cursor = '*'
        page_num = 1
        
        logger.info(f"å¼€å§‹é¡ºåºè·å– (ä¼˜åŒ–ï¼šper_page={per_page}, éµå®ˆå®˜æ–¹é™åˆ¶: 10 è¯·æ±‚/ç§’)")
        logger.warning("âš ï¸  OpenAlex ä¸æ”¯æŒå¹¶å‘æ¸¸æ ‡åˆ†é¡µï¼ˆä¼šå¯¼è‡´é‡å¤æ•°æ®ï¼‰ã€‚ä½¿ç”¨é¡ºåºæ¨¡å¼ã€‚")
        
        start_time = time.time()
        
        while cursor and len(all_results) < limit:
            try:
                params = {
                    'mailto': self.email,
                    'per_page': per_page,
                    'cursor': cursor,
                }
                
                if query:
                    params['search'] = query
                
                if filter_str:
                    params['filter'] = filter_str
                
                page_start = time.time()
                response = self.session.get(f"{self.BASE_URL}/works", params=params, timeout=30)
                page_elapsed = time.time() - page_start
                response.raise_for_status()
                
                data = response.json()
                results = data.get('results', [])
                meta = data.get('meta', {})
                cursor = meta.get('next_cursor')
                
                all_results.extend(results)
                
                logger.info(f"âœ“ ç¬¬ {page_num} é¡µ: {len(results)} ç¯‡ï¼ˆç´¯è®¡ {len(all_results)} ç¯‡ï¼‰ï¼Œè€—æ—¶ {page_elapsed:.1f}s")
                
                if len(all_results) >= limit:
                    logger.info(f"âœ“ å·²è¾¾åˆ°é™åˆ¶ {limit} ç¯‡è®ºæ–‡")
                    break
                
                if not cursor:
                    logger.info(f"âœ“ å·²è·å–å…¨éƒ¨æ•°æ®: å…± {len(all_results)} ç¯‡è®ºæ–‡")
                    break
                
                page_num += 1
            
            except requests.exceptions.Timeout:
                logger.warning(f"â± ç¬¬ {page_num} é¡µ: è¯·æ±‚è¶…æ—¶ï¼Œåœæ­¢è·å–")
                break
            
            except Exception as e:
                logger.error(f"âœ— ç¬¬ {page_num} é¡µ: å¤±è´¥: {str(e)}")
                break
        
        elapsed = time.time() - start_time
        logger.info(f"âœ“ è·å–å®Œæˆ: å…± {len(all_results)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ {elapsed:.1f}s")
        
        return all_results[:limit]

    def get_papers_by_year_range(
        self,
        year_min: int,
        year_max: int,
        limit: int = 1000,
        topic: Optional[str] = None,
        discipline: Optional[str] = None,
        institution: Optional[str] = None
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šå¹´ä»½èŒƒå›´çš„è®ºæ–‡æ•°æ®
        
        æ³¨æ„ï¼šåˆ†é¡µé€»è¾‘å·²åœ¨ search_works/_execute_search ä¸­å®ç°ï¼Œæ­¤æ–¹æ³•ç›´æ¥è°ƒç”¨ä¸€æ¬¡å³å¯è·å–æŒ‡å®šæ•°é‡çš„è®ºæ–‡
        """
        logger.info(f"è·å–è®ºæ–‡æ•°æ®: {year_min}-{year_max}, limit={limit}, topic={topic}, discipline={discipline}, institution={institution}")
        
        _discipline = discipline or topic
        
        # ç›´æ¥è°ƒç”¨ search_worksï¼Œå®ƒå†…éƒ¨ä¼šå¤„ç†åˆ†é¡µé€»è¾‘
        papers = self.search_works(
            year_min=year_min,
            year_max=year_max,
            limit=limit,  # ä¼ é€’å®Œæ•´çš„ limitï¼Œ_execute_search ä¼šå¤„ç†åˆ†é¡µ
            discipline=_discipline,
            institution=institution
        )
        
        logger.info(f"è·å–æˆåŠŸ: {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def get_work_by_id(self, work_id: str) -> Dict:
        """
        è·å–å•ç¯‡è®ºæ–‡è¯¦ç»†ä¿¡æ¯
        
        Args:
            work_id: è®ºæ–‡ ID (OpenAlex ID æˆ– DOI)
        
        Returns:
            è®ºæ–‡å¯¹è±¡
        
        æ–‡æ¡£: https://docs.openalex.org/how-to-use-the-api/get-single-entities
        """
        logger.info(f"è·å–è®ºæ–‡è¯¦æƒ…: {work_id}")
        
        params = {'mailto': self.email}
        
        try:
            response = self._make_request(f'/works/{work_id}', params=params)
            logger.info(f"âœ“ è·å–æˆåŠŸ")
            return response
        except Exception as e:
            logger.error(f"âœ— è·å–è®ºæ–‡å¤±è´¥: {e}")
            return {}
    
    def get_cited_by_works(self, work_id: str, limit: int = 100) -> List[Dict]:
        """
        è·å–å¼•ç”¨æŸè®ºæ–‡çš„å…¶ä»–è®ºæ–‡
        
        Args:
            work_id: è®ºæ–‡ ID
            limit: è¿”å›æ•°é‡é™åˆ¶
        
        Returns:
            å¼•ç”¨è¯¥è®ºæ–‡çš„è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"è·å–å¼•ç”¨è®ºæ–‡: {work_id}, limit={limit}")
        
        params = {
            'mailto': self.email,
            'per_page': min(200, limit),
            'filter': f'cites:{work_id}'
        }
        
        try:
            response = self._make_request('/works', params=params)
            works = response.get('results', [])
            
            logger.info(f"âœ“ è·å–æˆåŠŸ: {len(works)} ç¯‡è®ºæ–‡å¼•ç”¨äº†è¯¥è®ºæ–‡")
            
            return works[:limit]
        except Exception as e:
            logger.error(f"âœ— è·å–å¼•ç”¨è®ºæ–‡å¤±è´¥: {e}")
            return []
    
    def get_references_from_work(self, work_id: str, limit: int = 100) -> List[Dict]:
        """
        è·å–è®ºæ–‡å¼•ç”¨çš„å…¶ä»–è®ºæ–‡
        
        Args:
            work_id: è®ºæ–‡ ID
            limit: è¿”å›æ•°é‡é™åˆ¶
        
        Returns:
            è¯¥è®ºæ–‡å¼•ç”¨çš„è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"è·å–è®ºæ–‡å¼•ç”¨: {work_id}, limit={limit}")
        
        # é¦–å…ˆè·å–è®ºæ–‡å¯¹è±¡ä»¥è·å– referenced_works ä¿¡æ¯
        work = self.get_work_by_id(work_id)
        
        if not work or 'referenced_works' not in work:
            logger.warning(f"è®ºæ–‡æœªåŒ…å«å¼•ç”¨ä¿¡æ¯æˆ–ä¸å­˜åœ¨")
            return []
        
        referenced_ids = work.get('referenced_works', [])[:limit]
        referenced_works = []
        
        for ref_id in referenced_ids:
            try:
                ref_work = self.get_work_by_id(ref_id)
                if ref_work:
                    referenced_works.append(ref_work)
            except Exception as e:
                logger.debug(f"è·å–å¼•ç”¨è®ºæ–‡å¤±è´¥: {ref_id}, {e}")
                continue
        
        logger.info(f"âœ“ è·å–æˆåŠŸ: {len(referenced_works)} ç¯‡è¢«å¼•ç”¨çš„è®ºæ–‡")
        
        return referenced_works
    
    def get_authors_by_work_ids(self, work_ids: List[str]) -> List[Dict]:
        """
        æ ¹æ®è®ºæ–‡ ID åˆ—è¡¨æ‰¹é‡è·å–æ‰€æœ‰ä½œè€…ï¼ˆå®˜æ–¹ OR è¯­æ³•ä¼˜åŒ–ï¼‰
        
        ä½¿ç”¨ OpenAlex API çš„ OR è¯­æ³•åœ¨å•ä¸ªè¯·æ±‚ä¸­æ‰¹é‡æŸ¥è¯¢å¤šä¸ªè®ºæ–‡ IDï¼Œ
        è€Œä¸æ˜¯é€ä¸ªæŸ¥è¯¢ã€‚è¿™æ˜¯å®˜æ–¹æ¨èçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚
        
        å®˜æ–¹æ–‡æ¡£ï¼š
        - https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or
        - https://blog.ourresearch.org/fetch-multiple-dois-in-one-openalex-api-request/
        
        æ€§èƒ½å¯¹æ¯”ï¼š
        - é€ä¸ªæŸ¥è¯¢ï¼š100 ç¯‡è®ºæ–‡éœ€è¦ 100+ ä¸ªè¯·æ±‚
        - OR æ‰¹é‡ï¼š100 ç¯‡è®ºæ–‡åªéœ€ 2 ä¸ªè¯·æ±‚ âœ¨ (æ¯ä¸ªè¯·æ±‚æœ€å¤š 50 ç¯‡ï¼Œæ›´å®‰å…¨)
        
        Args:
            work_ids: è®ºæ–‡ ID åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å®Œæ•´ URL æˆ–çŸ­ IDï¼‰
        
        Returns:
            ä½œè€…åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        """
        if not work_ids:
            return []
        
        logger.info(f"æ‰¹é‡è·å– {len(work_ids)} ç¯‡è®ºæ–‡çš„ä½œè€…ä¿¡æ¯ï¼ˆä½¿ç”¨ OR è¯­æ³•æ‰¹é‡ä¼˜åŒ–ï¼‰")
        
        authors = {}  # ç”¨äºå»é‡
        
        # æå–çŸ­ IDï¼ˆå¦‚æœæ˜¯å®Œæ•´ URLï¼Œä»ä¸­æå– W... éƒ¨åˆ†ï¼‰
        short_ids = []
        for work_id in work_ids:
            if work_id.startswith('https://'):
                short_id = work_id.split('/')[-1]
            else:
                short_id = work_id
            short_ids.append(short_id)
        
        # åˆ†æ‰¹å¤„ç†ï¼ˆå®˜æ–¹é™åˆ¶ï¼šæœ€å¤š 100 ä¸ªå€¼/è¯·æ±‚ï¼Œå»ºè®® 50 æ›´å®‰å…¨ï¼‰
        batch_size = 20 if self.enable_or_batching else 1
        batches = self._batch_by_or_syntax(short_ids, batch_size)
        
        logger.info(f"åˆ† {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç† (æ¯æ‰¹æœ€å¤š {batch_size} ç¯‡)")
        
        for batch_idx, batch_ids in enumerate(batches, 1):
            try:
                # ä½¿ç”¨ç®¡é“ç¬¦ | æ„å»º OR è¿‡æ»¤å™¨
                if self.enable_or_batching and len(batch_ids) > 1:
                    id_filter = '|'.join(batch_ids)
                    params = {
                        'mailto': self.email,
                        'filter': f'openalex:{id_filter}',
                        'per_page': len(batch_ids),  # å¿…é¡»è®¾ç½®è¶³å¤Ÿå¤§çš„ per_page
                    }
                    logger.debug(f"æ‰¹æ¬¡ {batch_idx}/{len(batches)}: ä½¿ç”¨ OR è¯­æ³•æŸ¥è¯¢ {len(batch_ids)} ç¯‡è®ºæ–‡")
                else:
                    # å•ä¸ª IDï¼Œä¸ä½¿ç”¨ OR è¯­æ³•
                    params = {
                        'mailto': self.email,
                        'filter': f'openalex:{batch_ids[0]}',
                        'per_page': 1,
                    }
                    logger.debug(f"æ‰¹æ¬¡ {batch_idx}/{len(batches)}: æŸ¥è¯¢å•ç¯‡è®ºæ–‡")
                
                response = self._make_request('/works', params=params)
                works = response.get('results', [])
                
                logger.info(f"âœ“ æ‰¹æ¬¡ {batch_idx}/{len(batches)}: è·å– {len(works)} ç¯‡è®ºæ–‡çš„ä½œè€…")
                
                for work in works:
                    if 'authorships' in work:
                        for authorship in work['authorships']:
                            author_info = authorship.get('author', {})
                            author_id = author_info.get('id')
                            
                            if author_id and author_id not in authors:
                                authors[author_id] = {
                                    'id': author_id,
                                    'name': author_info.get('display_name'),
                                    'orcid': author_info.get('orcid'),
                                    'works_count': author_info.get('works_count'),
                                    'cited_by_count': author_info.get('cited_by_count'),
                                }
            except Exception as e:
                logger.warning(f"âœ— æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ“ è·å–å®Œæˆ: {len(authors)} ä½ä½œè€… (å…±å¤„ç† {len(short_ids)} ç¯‡è®ºæ–‡)")
        
        return list(authors.values())
    
    def get_collaboration_by_authors_batch(self, author_ids: List[str], max_papers_per_batch: int = 20000) -> List[Dict]:
        """
        æ‰¹é‡è·å–ä½œè€…ä¹‹é—´çš„åˆä½œå…³ç³» - ä½¿ç”¨ OR è¯­æ³•ä¼˜åŒ–ï¼ˆæ¨èï¼‰
        
        â­ æ¨èç”¨äºå¤§è§„æ¨¡ä½œè€…é›†åˆï¼ˆ>100 ä½ä½œè€…ï¼‰
        
        ğŸ“š å®˜æ–¹æé€Ÿæ–¹æ¡ˆï¼ˆOpenAlex æ–‡æ¡£ï¼‰ï¼š
        https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication
        
        1. **Polite Pool**ï¼ˆå·²å®ç°ï¼‰ï¼šæ·»åŠ  mailto å‚æ•°è·å¾—æ›´ç¨³å®šçš„å“åº”æ—¶é—´ âœ“
        2. **OR è¯­æ³•æ‰¹é‡è¯·æ±‚**ï¼ˆæœ¬æ–¹æ³•å®ç°ï¼‰ï¼š50 ä¸ªè¯·æ±‚å‹ç¼©æˆ 1 ä¸ª âœ“
        3. **Group By èšåˆ**ï¼ˆå¯é€‰è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰ï¼š
           - å¦‚æœåªéœ€è¦ç»Ÿè®¡æ•°ï¼Œä¸éœ€è¦è®ºæ–‡è¯¦æƒ…ï¼Œä½¿ç”¨ group_by=authorships.authors.id
           - å¯ç›´æ¥è¿”å›å„ä½œè€…çš„åˆä½œè®¡æ•°ç»Ÿè®¡ï¼Œé¿å…é€è®ºæ–‡è¿­ä»£
           - å®˜æ–¹ç¤ºä¾‹ï¼šhttps://blog.ourresearch.org/fetch-multiple-dois-in-one-openalex-api-request/
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨ OR è¯­æ³•æ‰¹é‡æŸ¥è¯¢å¤šå¯¹ä½œè€…ï¼š`author.id:A|B,author.id:C|D,author.id:E|F`
        2. æ¯ä¸ªè¯·æ±‚æŸ¥è¯¢å¤šå¯¹ä½œè€…ç»„åˆï¼Œå¤§å¹…å‡å°‘è¯·æ±‚æ•°
        3. æ€§èƒ½æå‡ï¼š425503 å¯¹ â†’ ä» 95000s é™åˆ° ~1000sï¼ˆç›¸æ¯”é€å¯¹æŸ¥è¯¢ï¼‰
        4. ä¸¥æ ¼éµå®ˆ 10 req/s çš„é€Ÿç‡é™åˆ¶
        
        æ€§èƒ½æ³¨æ„äº‹é¡¹ï¼š
        - è¿”å›çš„è®ºæ–‡æ•°é‡å†³å®šäº†åˆ†é¡µæ¬¡æ•°ï¼ˆæ¯é¡µ 200 æ¡ï¼‰
        - æ´»è·ƒä½œè€…çš„åˆä½œè®ºæ–‡å¯èƒ½å¾ˆå¤šï¼Œå¯¼è‡´å¤šæ¬¡åˆ†é¡µ
        - å¦‚æœæ€»è®ºæ–‡æ•° > max_papers_per_batchï¼Œä¼šé€šè¿‡åˆ†é¡µè·å–æ‰€æœ‰æ•°æ®ï¼ˆæ—©æœŸåœæ­¢ï¼‰
        - ä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼Œé»˜è®¤é™åˆ¶ä¸º 2000 ç¯‡è®ºæ–‡/æ‰¹æ¬¡ï¼ˆçº¦ 10 é¡µï¼‰
        - å¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼šæ›´å°å€¼ = æ›´å¿«ä½†å¯èƒ½é”™è¿‡æŸäº›åˆä½œï¼Œæ›´å¤§å€¼ = æ›´æ…¢ä½†æ›´å®Œæ•´
        
        å·¥ä½œåŸç†ï¼š
        - å°†æ‰€æœ‰ä½œè€…åˆ†æˆä¸¤ç»„ï¼šgroup_a å’Œ group_b
        - ä½¿ç”¨ OR æ‰¹é‡æŸ¥è¯¢ï¼š`author.id:{group_a},author.id:{group_b}`
        - æ¯ä¸ªè¯·æ±‚è¿”å›åŒæ—¶åŒ…å« group_a å’Œ group_b ä¸­ä»»æ„ä½œè€…çš„è®ºæ–‡
        - ç»Ÿè®¡æ¯å¯¹ä½œè€…çš„åˆä½œè®ºæ–‡æ•°é‡
        
        æ€§èƒ½å¯¹æ¯”ï¼š
        - é€å¯¹æŸ¥è¯¢ï¼ˆget_collaboration_by_authorsï¼‰ï¼š425503 å¯¹ = 425503 ä¸ªè¯·æ±‚
        - æ‰¹é‡æŸ¥è¯¢ï¼ˆget_collaboration_by_authors_batchï¼‰ï¼š425503 å¯¹ â‰ˆ 100-200 ä¸ªè¯·æ±‚ âœ¨
        
        å®˜æ–¹æ–‡æ¡£ï¼š
        - https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or
        - https://blog.ourresearch.org/fetch-multiple-dois-in-one-openalex-api-request/
        
        Args:
            author_ids: ä½œè€… ID åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å®Œæ•´ URL æˆ–çŸ­ IDï¼‰
            max_papers_per_batch: å•ä¸ªæ‰¹æ¬¡æŸ¥è¯¢çš„æœ€å¤§è®ºæ–‡æ•°ï¼ˆé˜²æ­¢è¿‡å¤šåˆ†é¡µï¼Œé»˜è®¤ 2000ï¼‰
                                 - 2000ï¼ˆé»˜è®¤ï¼‰ï¼šçº¦ 10 é¡µï¼Œå¹³è¡¡é€Ÿåº¦å’Œå®Œæ•´æ€§
                                 - 1000ï¼šçº¦ 5 é¡µï¼Œæ›´å¿«ä½†å¯èƒ½é—æ¼éƒ¨åˆ†åˆä½œ
                                 - 5000+ï¼šæ›´å®Œæ•´ä½†ä¼šå¯¼è‡´åˆ†é¡µå¾ˆå¤š
        
        Returns:
            åˆä½œå…³ç³»åˆ—è¡¨ [{'from': author_id, 'to': author_id, 'weight': num_collaborations}]
        """
        if not author_ids:
            return []
        
        # æå–çŸ­ ID
        short_ids = []
        full_ids = {}
        for author_id in author_ids:
            if author_id.startswith('https://'):
                short_id = author_id.split('/')[-1]
            else:
                short_id = author_id
            short_ids.append(short_id)
            full_ids[short_id] = author_id
        
        num_authors = len(short_ids)
        num_pairs = num_authors * (num_authors - 1) // 2
        
        logger.info(f"è·å– {num_authors} ä½ä½œè€…çš„åˆä½œå…³ç³»ï¼ˆæ‰¹é‡ OR è¯­æ³•æ¨¡å¼ï¼‰")
        logger.info(f"æ€»å…± {num_pairs} å¯¹ä½œè€…ç»„åˆ")
        
        # è‡ªé€‚åº” batch_size é€‰æ‹©ï¼ˆæ ¹æ®ä½œè€…æ•°é‡ä¼˜åŒ–æ€§èƒ½ï¼‰
        batch_size = self._get_optimal_batch_size(num_authors)
        
        # ä¼°ç®—è¯·æ±‚æ•°ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
        # åˆ†åˆ«æŸ¥è¯¢ author.id:A|B|C...ï¼ˆåˆ†ç»„1ï¼‰å’Œ author.id:D|E|F...ï¼ˆåˆ†ç»„2ï¼‰
        # æ¯ä¸ªè¯·æ±‚æŸ¥è¯¢å¤šå¯¹ï¼Œè€Œä¸æ˜¯å•å¯¹
        # æ³¨æ„ï¼šå®é™…è¯·æ±‚æ•° = æ‰¹æ¬¡ç»„åˆæ•° Ã— æ¯ä¸ªæ‰¹æ¬¡çš„åˆ†é¡µæ•°é‡ï¼ˆå–å†³äºè¿”å›çš„è®ºæ–‡æ•°ï¼‰
        num_batches = (num_authors + batch_size - 1) // batch_size  # ceil é™¤æ³•
        estimated_requests = num_batches * (num_batches + 1) // 2
        estimated_time = estimated_requests * (0.1 + batch_size * 0.01)  # ä¼°ç®—è€—æ—¶
        
        logger.info(f"âœ“ è‡ªé€‚åº” batch_size: {batch_size}")
        logger.info(f"  é¢„æœŸæ‰¹æ¬¡æ•°: {num_batches}")
        logger.info(f"  é¢„æœŸè¯·æ±‚æ•°: ~{estimated_requests} ä¸ªï¼ˆvs {num_pairs} ä¸ªé€å¯¹æŸ¥è¯¢ï¼‰")
        logger.info(f"  æ€§èƒ½æå‡: ~{num_pairs / estimated_requests:.0f}x")
        logger.info(f"  é¢„æœŸè€—æ—¶: ~{estimated_time:.0f}s")
        logger.warning(f"âš ï¸  æç¤ºï¼šå®é™…è¯·æ±‚æ•°å¯èƒ½æ›´å¤šï¼Œå–å†³äºæ¯ä¸ªæ‰¹æ¬¡æŸ¥è¯¢è¿”å›çš„è®ºæ–‡æ•°é‡å’Œåˆ†é¡µæƒ…å†µ")
        
        collaborations = {}
        start_time = time.time()
        
        # ä½¿ç”¨ä¸¤å±‚å¾ªç¯ï¼šæ¯å±‚å¤„ç†ä¸€æ‰¹ä½œè€…
        # è¿™æ ·å¯ä»¥ç”¨ OR è¯­æ³•åœ¨å•ä¸ªè¯·æ±‚ä¸­æŸ¥è¯¢å¤šå¯¹ä½œè€…
        processed_pairs = 0
        total_requests = 0
        
        # å°†ä½œè€…åˆ†æˆå¤šæ‰¹
        batches = self._batch_by_or_syntax(short_ids, batch_size)
        
        logger.info(f"åˆ† {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹ {batch_size} ä½ä½œè€…ï¼‰")
        
        for batch_a_idx, batch_a_ids in enumerate(batches):
            for batch_b_idx, batch_b_ids in enumerate(batches):
                # åªæŸ¥è¯¢ batch_a_idx <= batch_b_idxï¼Œé¿å…é‡å¤æŸ¥è¯¢
                if batch_a_idx > batch_b_idx:
                    continue
                
                total_requests += 1
                
                try:
                    # æ„å»º OR è¯­æ³•è¿‡æ»¤å™¨
                    # author.id:A|B|C,author.id:D|E|F è¡¨ç¤º (Aæˆ–Bæˆ–C) ä¸” (Dæˆ–Eæˆ–F)
                    group_a = '|'.join(batch_a_ids)
                    group_b = '|'.join(batch_b_ids)
                    
                    # å¦‚æœæ˜¯åŒä¸€ä¸ªæ‰¹æ¬¡ï¼Œç‰¹æ®Šå¤„ç†é¿å…è‡ªå·±å’Œè‡ªå·±é…å¯¹
                    if batch_a_idx == batch_b_idx:
                        # å•ä¸ªæ‰¹æ¬¡å†…çš„åˆä½œï¼šæŸ¥è¯¢è¯¥æ‰¹æ¬¡ä¸­æ‰€æœ‰ä½œè€…çš„è®ºæ–‡
                        # ç„¶ååœ¨ä»£ç ä¸­ç»Ÿè®¡å…¶ä¸­åŒ…å« 2 ä¸ªæˆ–ä»¥ä¸Šè¯¥æ‰¹æ¬¡ä½œè€…çš„è®ºæ–‡
                        filter_str = f'author.id:{group_a}'
                        params = {
                            'mailto': self.email,
                            'filter': filter_str,
                            'per_page': 200,  # æœ€å¤§å€¼ 200ï¼ˆOpenAlex é™åˆ¶ï¼‰
                            'cursor': '*'
                        }
                        
                        logger.debug(f"è¯·æ±‚ {total_requests}: æ‰¹æ¬¡ ({batch_a_idx}) å†…éƒ¨åˆä½œ")
                    else:
                        # ä¸åŒæ‰¹æ¬¡é—´çš„åˆä½œï¼šæŸ¥è¯¢åŒæ—¶åŒ…å«ä¸¤ä¸ªæ‰¹æ¬¡ä¸­çš„ä½œè€…çš„è®ºæ–‡
                        filter_str = f'author.id:{group_a},author.id:{group_b}'
                        params = {
                            'mailto': self.email,
                            'filter': filter_str,
                            'per_page': 200,  # æœ€å¤§å€¼ 200ï¼ˆOpenAlex é™åˆ¶ï¼‰
                            'cursor': '*'
                        }
                        
                        logger.debug(f"è¯·æ±‚ {total_requests}: æ‰¹æ¬¡ ({batch_a_idx}) Ã— ({batch_b_idx}) çš„åˆä½œ")
                    
                    # è·å–æ‰€æœ‰åŒ¹é…çš„è®ºæ–‡
                    all_works = []
                    cursor = '*'
                    page_count = 0
                    api_call_count = 0  # ç»Ÿè®¡å®é™… API è°ƒç”¨æ¬¡æ•°
                    
                    while cursor:
                        page_count += 1
                        params['cursor'] = cursor
                        api_call_count += 1
                        
                        try:
                            response = self._make_request('/works', params=params, timeout=60, max_retries=2)
                        except Exception as e:
                            logger.warning(f"  æ‰¹æ¬¡æŸ¥è¯¢å¤±è´¥: {str(e)[:50]}")
                            break
                        
                        works = response.get('results', [])
                        meta = response.get('meta', {})
                        cursor = meta.get('next_cursor')
                        total_count = meta.get('count', 0)
                        
                        all_works.extend(works)
                        
                        # å¦‚æœé¦–æ¬¡å‘ç°æ•°æ®å¾ˆå¤šï¼Œè¾“å‡ºè¯Šæ–­ä¿¡æ¯
                        if page_count == 1 and total_count > 500:
                            estimated_pages = (total_count + 199) // 200  # å‘ä¸Šå–æ•´
                            logger.info(f"  ğŸ’¡ æ‰¹æ¬¡æŸ¥è¯¢è¿”å› {total_count} ç¯‡è®ºæ–‡ï¼Œé¢„è®¡éœ€è¦ ~{estimated_pages} é¡µåˆ†é¡µ")
                            if estimated_pages > 15:
                                logger.warning(f"  âš ï¸ è­¦å‘Šï¼šåˆ†é¡µæ¬¡æ•°å¾ˆå¤šï¼å¯èƒ½éœ€è¦ {estimated_pages * 0.5:.0f}+ ç§’")
                        
                        # å¦‚æœåˆ†é¡µæ¬¡æ•°è¿‡å¤šï¼Œè¾“å‡ºè­¦å‘Šå’Œå»ºè®®
                        if page_count > 10:
                            logger.warning(f"  âš ï¸ åˆ†é¡µæ¬¡æ•°è¿‡å¤š ({page_count} é¡µ)ï¼Œå·²è€—æ—¶ {api_call_count * 0.5:.0f}+ ç§’")
                            logger.warning(f"     ğŸ’¡ å»ºè®®ï¼š")
                            logger.warning(f"        1. å‡å° batch_sizeï¼ˆå½“å‰ä¼šæ›´å¤šåˆ†é¡µï¼‰")
                            logger.warning(f"        2. å‡å° max_papers_per_batch å‚æ•°ï¼ˆæ—©æœŸåœæ­¢ï¼‰")
                            logger.warning(f"        3. æ£€æŸ¥æ˜¯å¦ä½œè€…åŒ…å«é«˜äº§ç ”ç©¶è€…ï¼ˆè®ºæ–‡æ•°å¾ˆå¤šï¼‰")
                        
                        # æ—©æœŸåœæ­¢ï¼šå¦‚æœè·å–çš„è®ºæ–‡æ•°è¶…è¿‡é™åˆ¶ï¼Œåœæ­¢åˆ†é¡µ
                        if len(all_works) > max_papers_per_batch:
                            logger.warning(f"  ğŸ’¡ å·²è·å– {len(all_works)} ç¯‡è®ºæ–‡ï¼ˆè¶…è¿‡é™åˆ¶ {max_papers_per_batch}ï¼‰ï¼Œåœæ­¢åˆ†é¡µä»¥åŠ å¿«å¤„ç†")
                            break
                    
                    # ç»Ÿè®¡è¯¥æ‰¹æ¬¡ä¸­çš„ä½œè€…å¯¹åˆä½œè®ºæ–‡æ•°
                    if batch_a_idx == batch_b_idx:
                        # å•æ‰¹æ¬¡ï¼šç»Ÿè®¡æ‰€æœ‰ä½œè€…å¯¹çš„åˆä½œ
                        author_pair_works = {}
                        
                        for work in all_works:
                            work_authors = []
                            if 'authorships' in work:
                                for auth in work['authorships']:
                                    author_id = auth.get('author', {}).get('id')
                                    if author_id:
                                        # æå–çŸ­ ID
                                        short_id = author_id.split('/')[-1] if '/' in author_id else author_id
                                        if short_id in batch_a_ids:
                                            work_authors.append(short_id)
                            
                            # ç»Ÿè®¡è¯¥è®ºæ–‡ä¸­å‡ºç°çš„æ‰€æœ‰ä½œè€…å¯¹
                            for i in range(len(work_authors)):
                                for j in range(i + 1, len(work_authors)):
                                    pair = tuple(sorted([work_authors[i], work_authors[j]]))
                                    author_pair_works[pair] = author_pair_works.get(pair, 0) + 1
                        
                        # æ·»åŠ åˆ°åˆä½œå…³ç³»å­—å…¸
                        for (short_a, short_b), count in author_pair_works.items():
                            full_a = full_ids[short_a]
                            full_b = full_ids[short_b]
                            key = tuple(sorted([full_a, full_b]))
                            
                            if key not in collaborations:
                                collaborations[key] = {
                                    'from': key[0],
                                    'to': key[1],
                                    'weight': count
                                }
                            processed_pairs += 1
                    else:
                        # ä¸åŒæ‰¹æ¬¡ï¼šç»Ÿè®¡è·¨æ‰¹æ¬¡ä½œè€…å¯¹çš„åˆä½œ
                        author_pair_works = {}
                        
                        for work in all_works:
                            work_authors_a = []
                            work_authors_b = []
                            
                            if 'authorships' in work:
                                for auth in work['authorships']:
                                    author_id = auth.get('author', {}).get('id')
                                    if author_id:
                                        short_id = author_id.split('/')[-1] if '/' in author_id else author_id
                                        if short_id in batch_a_ids:
                                            work_authors_a.append(short_id)
                                        if short_id in batch_b_ids:
                                            work_authors_b.append(short_id)
                            
                            # ç»Ÿè®¡è¯¥è®ºæ–‡ä¸­ batch_a å’Œ batch_b çš„ä½œè€…å¯¹
                            for short_a in work_authors_a:
                                for short_b in work_authors_b:
                                    if short_a != short_b:  # é¿å…åŒä¸€ä½œè€…é…å¯¹
                                        pair = tuple(sorted([short_a, short_b]))
                                        author_pair_works[pair] = author_pair_works.get(pair, 0) + 1
                        
                        # æ·»åŠ åˆ°åˆä½œå…³ç³»å­—å…¸
                        for (short_a, short_b), count in author_pair_works.items():
                            full_a = full_ids[short_a]
                            full_b = full_ids[short_b]
                            key = tuple(sorted([full_a, full_b]))
                            
                            if key not in collaborations:
                                collaborations[key] = {
                                    'from': key[0],
                                    'to': key[1],
                                    'weight': count
                                }
                            processed_pairs += 1
                    
                    logger.info(f"âœ“ è¯·æ±‚ {total_requests}: æ‰¾åˆ° {len(all_works)} ç¯‡è®ºæ–‡ï¼ˆ{api_call_count} ä¸ª API è°ƒç”¨ï¼Œ{page_count} é¡µï¼‰ï¼Œ{len(author_pair_works)} å¯¹ä½œè€…åˆä½œ")
                
                except Exception as e:
                    logger.warning(f"âœ— è¯·æ±‚ {total_requests} å¤±è´¥: {str(e)[:80]}")
                    continue
        
        elapsed = time.time() - start_time
        
        # ç»Ÿè®¡çœŸå®çš„ API è°ƒç”¨æ•°
        actual_api_calls = sum(len([w for w in all_works]) for _ in [None])  # è¿™é‡Œéœ€è¦ä»æ—¥å¿—æ¨æ–­
        
        logger.info(f"âœ“ è·å–å®Œæˆ:")
        logger.info(f"  æ‰¹æ¬¡æŸ¥è¯¢æ•°: {total_requests} ä¸ª")
        logger.info(f"  æ‰¾åˆ°åˆä½œå…³ç³»: {len(collaborations)} æ¡")
        logger.info(f"  è€—æ—¶: {elapsed:.1f}s")
        logger.info(f"  å¹³å‡è€—æ—¶: {elapsed / total_requests:.1f}s/æ‰¹æ¬¡")
        logger.info(f"  ")
        logger.info(f"  æ€§èƒ½ä¼˜åŒ–æç¤º:")
        if elapsed > 300:
            logger.warning(f"  â±ï¸  æ€»è€—æ—¶ > 5 åˆ†é’Ÿï¼Œå»ºè®®ä¼˜åŒ–ï¼š")
            logger.warning(f"     1. å‡å° max_papers_per_batchï¼ˆå½“å‰é»˜è®¤ 2000ï¼‰")
            logger.warning(f"        get_collaboration_by_authors_batch(author_ids, max_papers_per_batch=1000)")
            logger.warning(f"     2. å‡å°ä½œè€…é›†åˆè§„æ¨¡æˆ–åˆ†å¤šæ¬¡æŸ¥è¯¢")
            logger.warning(f"     3. æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿæˆ– API æœåŠ¡çŠ¶æ€")
        logger.info(f"  ğŸ’¡ å®é™… API è°ƒç”¨æ•° = æ‰¹æ¬¡æŸ¥è¯¢æ•° Ã— è¯¥æ‰¹æ¬¡çš„åˆ†é¡µæ•°")
        logger.info(f"     å¯æ ¹æ®æ—¥å¿—ä¸­çš„é¡µæ•°ä¼°ç®—ï¼Œæ¯é¡µçº¦ 0.5 ç§’")
        
        return list(collaborations.values())
    
    def get_citation_network(
        self,
        year_min: int = 2020,
        year_max: int = 2024,
        limit: int = 500,
        discipline: Optional[str] = None,
        institution: Optional[str] = None
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        è·å–å¼•ç”¨ç½‘ç»œæ•°æ® - ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼é¿å…å†…å­˜æº¢å‡º
        
        æ³¨æ„ï¼šè¿”å› (nodes, edges_generator) å…¶ä¸­ edges_generator æ˜¯ç”Ÿæˆå™¨
        è°ƒç”¨è€…åº”è¯¥æµå¼æ¶ˆè´¹ç”Ÿæˆå™¨ï¼Œä¸è¦ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è¾¹åˆ°å†…å­˜
        """
        
        logger.info(f"è·å–å¼•ç”¨ç½‘ç»œ: {year_min}-{year_max}, limit={limit}, discipline={discipline}, institution={institution}")
        
        # ç›´æ¥ç”¨è¿‡æ»¤å‚æ•°æŸ¥è¯¢
        papers = self.search_works(
            query=None,
            year_min=year_min,
            year_max=year_max,
            limit=limit,
            discipline=discipline,
            institution=institution
        )
        
        if not papers:
            logger.warning("æœªè·å–åˆ°è®ºæ–‡")
            return [], []
        
        logger.info(f"è·å–æˆåŠŸ: {len(papers)} ç¯‡è®ºæ–‡ï¼Œå‡†å¤‡æµå¼å¤„ç†å¼•ç”¨å…³ç³»...")
        
        nodes = papers
        
        # åˆ›å»ºè¾¹ç”Ÿæˆå™¨å‡½æ•° - å»¶è¿Ÿè®¡ç®—ï¼Œä¸å ç”¨å†…å­˜
        def edges_generator():
            """ç”Ÿæˆå¼•ç”¨è¾¹ - é€ä¸ªè®ºæ–‡é€ä¸ªå¼•ç”¨ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜"""
            total_refs = 0
            for paper_idx, paper in enumerate(nodes, 1):
                referenced_works = paper.get('referenced_works', [])
                if not referenced_works:
                    continue
                
                for ref_id in referenced_works:
                    if ref_id:
                        yield {
                            'source': paper.get('id', ''),
                            'target': ref_id,
                            'weight': 1
                        }
                        total_refs += 1
                
                # æ¯å¤„ç† 5000 ç¯‡è®ºæ–‡ï¼Œè¾“å‡ºä¸€æ¬¡è¿›åº¦
                if paper_idx % 5000 == 0:
                    logger.info(f"å¤„ç†è¿›åº¦: {paper_idx}/{len(papers)} ç¯‡è®ºæ–‡ï¼Œå·²ç”Ÿæˆ {total_refs} æ¡å¼•ç”¨å…³ç³»")
            
            logger.info(f"âœ“ å¼•ç”¨å…³ç³»ç”Ÿæˆå®Œæ¯•: å…± {total_refs} æ¡è¾¹")
        
        # ä¼°ç®—æ€»çš„å¼•ç”¨å…³ç³»æ•°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        estimated_edges = sum(len(p.get('referenced_works', [])) for p in nodes)
        logger.info(f"âœ“ å‡†å¤‡å¥½å¼•ç”¨ç½‘ç»œ: {len(nodes)} ä¸ªèŠ‚ç‚¹, é¢„æœŸçº¦ {estimated_edges} æ¡è¾¹")
        logger.info(f"  å†…å­˜å ç”¨ä¼°ç®—: çº¦ {estimated_edges * 0.0002:.1f} MB (æ¯æ¡è¾¹ ~200 å­—èŠ‚)")
        
        # è¿”å›èŠ‚ç‚¹åˆ—è¡¨å’Œè¾¹ç”Ÿæˆå™¨ï¼ˆä¸æ˜¯åˆ—è¡¨ï¼ï¼‰
        return nodes, edges_generator()
    
    def search_institutions(self, query: str, limit: int = 20) -> List[Dict]:
        """
        æœç´¢æœºæ„/å¤§å­¦
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼ˆå¤§å­¦åç§°æˆ–åŸå¸‚ç­‰ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        
        Returns:
            æœºæ„åˆ—è¡¨ï¼ŒåŒ…å« OpenAlex ID å’Œå…¶ä»–ä¿¡æ¯
        """
        logger.info(f"æœç´¢æœºæ„: {query}, limit={limit}")
        
        try:
            params = {
                'mailto': self.email,
                'search': query,
                'per_page': min(50, limit),
            }
            
            response = self._make_request('/institutions', params=params)
            institutions = response.get('results', [])
            
            result = []
            for inst in institutions[:limit]:
                result.append({
                    'id': inst.get('id'),
                    'display_name': inst.get('display_name'),
                    'country_code': inst.get('country_code'),
                    'country': inst.get('country'),
                    'type': inst.get('type'),
                    'works_count': inst.get('works_count'),
                    'cited_by_count': inst.get('cited_by_count'),
                    'ror_id': inst.get('ror'),
                })
            
            logger.info(f"âœ“ æœç´¢æˆåŠŸ: æ‰¾åˆ° {len(result)} ä¸ªæœºæ„")
            return result
        except Exception as e:
            logger.error(f"âœ— æœç´¢æœºæ„å¤±è´¥: {e}")
            return []
    
    def search_topics(self, query: str, limit: int = 20) -> List[Dict]:
        """
        æœç´¢ä¸»é¢˜/å­¦ç§‘
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼ˆå­¦ç§‘åç§°ç­‰ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        
        Returns:
            ä¸»é¢˜åˆ—è¡¨ï¼ŒåŒ…å« OpenAlex ID å’Œå…¶ä»–ä¿¡æ¯
        """
        logger.info(f"æœç´¢ä¸»é¢˜: {query}, limit={limit}")
        
        try:
            params = {
                'mailto': self.email,
                'search': query,
                'per_page': min(50, limit),
            }
            
            response = self._make_request('/topics', params=params)
            topics = response.get('results', [])
            
            result = []
            for topic in topics[:limit]:
                result.append({
                    'id': topic.get('id'),
                    'display_name': topic.get('display_name'),
                    'description': topic.get('description'),
                    'keywords': topic.get('keywords', []),
                    'subfield': topic.get('subfield', {}),
                    'field': topic.get('field', {}),
                    'works_count': topic.get('works_count'),
                    'cited_by_count': topic.get('cited_by_count'),
                })
            
            logger.info(f"âœ“ æœç´¢æˆåŠŸ: æ‰¾åˆ° {len(result)} ä¸ªä¸»é¢˜")
            return result
        except Exception as e:
            logger.error(f"âœ— æœç´¢ä¸»é¢˜å¤±è´¥: {e}")
            return []
        """
        å°†è®ºæ–‡å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸º DataFrame
        
        Args:
            works: è®ºæ–‡å¯¹è±¡åˆ—è¡¨
        
        Returns:
            è®ºæ–‡ DataFrame
        """
        if not works:
            return pd.DataFrame()
        
        records = []
        
        for work in works:
            # å¤„ç†ä½œè€…åˆ—è¡¨
            authors = []
            if 'authorships' in work:
                authors = [auth.get('author', {}).get('display_name', 'Unknown') 
                          for auth in work['authorships']]
            
            records.append({
                'id': work.get('id'),
                'title': work.get('title'),
                'year': work.get('publication_year'),
                'authors': '; '.join(authors),
                'venue': work.get('primary_location', {}).get('source', {}).get('display_name'),
                'cited_by_count': work.get('cited_by_count', 0),
                'abstract': work.get('abstract'),
                'doi': work.get('doi'),
                'url': work.get('id'),
            })
        
        df = pd.DataFrame(records)
        logger.info(f"âœ“ è½¬æ¢æˆåŠŸ: {len(df)} æ¡è®°å½•")
        
        return df


class DataCache:
    """
    æ•°æ®ç¼“å­˜ç±» - æ”¯æŒå†…å­˜ç¼“å­˜å’ŒRedisç¼“å­˜
    """
    def __init__(self, use_redis: bool = False, redis_host: str = 'localhost', redis_port: int = 6379, ttl: int = 3600):
        """
        åˆå§‹åŒ–ç¼“å­˜
        
        Args:
            use_redis: æ˜¯å¦ä½¿ç”¨Redis
            redis_host: Redisä¸»æœº
            redis_port: Redisç«¯å£
            ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        """
        self.use_redis = use_redis
        self.ttl = ttl
        self.memory_cache = {}
        self.redis_client = None
        
        if use_redis:
            try:
                import redis
                self.redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
                self.redis_client.ping()
                logger.info("âœ“ Redis è¿æ¥æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Redis è¿æ¥å¤±è´¥ï¼Œæ”¹ä¸ºä½¿ç”¨å†…å­˜ç¼“å­˜: {e}")
                self.use_redis = False
    
    def get(self, key: str):
        """è·å–ç¼“å­˜å€¼"""
        try:
            if self.use_redis and self.redis_client:
                value = self.redis_client.get(key)
                if value:
                    import json
                    logger.debug(f"ç¼“å­˜å‘½ä¸­ (Redis): {key}")
                    return json.loads(value)
            else:
                if key in self.memory_cache:
                    logger.debug(f"ç¼“å­˜å‘½ä¸­ (å†…å­˜): {key}")
                    return self.memory_cache[key]
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            if self.use_redis and self.redis_client:
                import json
                self.redis_client.setex(key, self.ttl, json.dumps(value))
                logger.debug(f"ç¼“å­˜è®¾ç½® (Redis): {key}")
            else:
                self.memory_cache[key] = value
                logger.debug(f"ç¼“å­˜è®¾ç½® (å†…å­˜): {key}")
        except Exception as e:
            logger.warning(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            if self.use_redis and self.redis_client:
                self.redis_client.flushdb()
                logger.info("âœ“ Redis ç¼“å­˜å·²æ¸…ç©º")
            else:
                self.memory_cache.clear()
                logger.info("âœ“ å†…å­˜ç¼“å­˜å·²æ¸…ç©º")
        except Exception as e:
            logger.warning(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")


# åœ¨æ–‡ä»¶æœ«å°¾åˆ›å»ºå…¨å±€å•ä¾‹å®ä¾‹
_fetcher_instance = None

def get_fetcher():
    """è·å– OpenAlexFetcher å•ä¾‹å®ä¾‹"""
    global _fetcher_instance
    if _fetcher_instance is None:
        _fetcher_instance = OpenAlexFetcher()
    return _fetcher_instance
