# UFCT Backend - æ•°æ®å¤„ç†è¯¦è§£æŒ‡å—

## ğŸ“– ç›®å½•

1. [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
2. [æ•°æ®å¤„ç†æµç¨‹](#æ•°æ®å¤„ç†æµç¨‹)
3. [å†…å­˜ç®¡ç†ä¼˜åŒ–](#å†…å­˜ç®¡ç†ä¼˜åŒ–)
4. [å¹¶å‘æ§åˆ¶æœºåˆ¶](#å¹¶å‘æ§åˆ¶æœºåˆ¶)
5. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
6. [API é€Ÿç‡é™åˆ¶](#api-é€Ÿç‡é™åˆ¶)
7. [æ‰¹é‡å¤„ç†ä¼˜åŒ–](#æ‰¹é‡å¤„ç†ä¼˜åŒ–)
8. [ç¼“å­˜æœºåˆ¶](#ç¼“å­˜æœºåˆ¶)
9. [æ•…éšœå¤„ç†ä¸é‡è¯•](#æ•…éšœå¤„ç†ä¸é‡è¯•)
10. [ç›‘æ§å’Œæ—¥å¿—](#ç›‘æ§å’Œæ—¥å¿—)

---

## ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### æ•´ä½“æ•°æ®æµ

```
ç”¨æˆ·è¯·æ±‚
    â†“
Flask API å±‚ (routes.py)
    â†“
Service å±‚ (xxxxx_service.py) - ä¸šåŠ¡é€»è¾‘å¤„ç†
    â†“
Repository å±‚ (xxxxx_repository.py) - æ•°æ®è®¿é—®
    â†“
OpenAlex æ•°æ®æº / æœ¬åœ°ç¼“å­˜
    â†“
æ•°æ®å¤„ç†å±‚ (openalex_fetcher.py) - çœŸå®çš„æ•°æ®è·å–å’Œä¼˜åŒ–
    â†“
è¿”å›æ ‡å‡†åŒ–æ•°æ®
```

### æ ¸å¿ƒæ¨¡å—å…³ç³»å›¾

```
openalex_fetcher.py (æ•°æ®è·å–æ ¸å¿ƒ)
â”œâ”€â”€ RateLimiter (é€Ÿç‡é™åˆ¶)
â”‚   â””â”€â”€ ä»¤ç‰Œæ¡¶ç®—æ³•ï¼Œæ§åˆ¶è¯·æ±‚é¢‘ç‡
â”œâ”€â”€ OpenAlexFetcher (ä¸»è·å–å™¨)
â”‚   â”œâ”€â”€ _make_request() - å¸¦é‡è¯•çš„ HTTP è¯·æ±‚
â”‚   â”œâ”€â”€ _rate_limit() - é€Ÿç‡é™åˆ¶æ‰§è¡Œ
â”‚   â”œâ”€â”€ _batch_by_or_syntax() - æ‰¹é‡åˆ†ç»„
â”‚   â”œâ”€â”€ search_works() - è®ºæ–‡æœç´¢
â”‚   â”œâ”€â”€ get_authors_by_work_ids() - è·å–ä½œè€…ï¼ˆOR ä¼˜åŒ–ï¼‰
â”‚   â””â”€â”€ get_collaboration_by_authors_batch() - æ‰¹é‡åˆä½œå…³ç³»
â””â”€â”€ å‚æ•°éªŒè¯å’Œè½¬æ¢å·¥å…·

æ•°æ®ç¼“å­˜å±‚
â”œâ”€â”€ å†…å­˜ç¼“å­˜ (Python dict)
â”œâ”€â”€ Redis ç¼“å­˜ (å¯é€‰)
â””â”€â”€ TTL ç®¡ç†

Service å±‚
â”œâ”€â”€ AuthorService
â”œâ”€â”€ PaperService
â”œâ”€â”€ NetworkService
â””â”€â”€ StatisticsService
```

---

## æ•°æ®å¤„ç†æµç¨‹

### 1. è®ºæ–‡æœç´¢æµç¨‹

```python
search_works(query, year_min, year_max, discipline, limit)
    â†“
å‚æ•°éªŒè¯ (OpenAlexParamValidator)
    â”œâ”€ å­¦ç§‘ ID è½¬æ¢ (CS â†’ T13674)
    â”œâ”€ æœºæ„ ID è½¬æ¢ (å­¦æ ¡å â†’ ROR ID)
    â””â”€ å¹´ä»½èŒƒå›´éªŒè¯
    â†“
æ„å»ºè¿‡æ»¤æ¡ä»¶ (filter_str)
    â”œâ”€ publication_year:2020-2024
    â”œâ”€ topics.id:T13674|T10470|...  (å¤šå­¦ç§‘ OR æŸ¥è¯¢)
    â””â”€ authorships.institutions.id:I...
    â†“
_execute_search() - åˆ†é¡µè·å–è®ºæ–‡
    â”œâ”€ per_page=200 (æœ€å¤§åŒ–æ¯é¡µæ•°é‡)
    â”œâ”€ ä½¿ç”¨ cursor åˆ†é¡µ (keyset pagination)
    â”œâ”€ ç´¯ç§¯ç»“æœç›´åˆ°è¾¾åˆ° limit
    â””â”€ éµå®ˆ 10 req/s é€Ÿç‡é™åˆ¶
    â†“
_normalize_works() - æ ‡å‡†åŒ–
    â”œâ”€ éªŒè¯æ¯ç¯‡è®ºæ–‡çš„ ID
    â”œâ”€ è¿‡æ»¤æ— æ•ˆæ•°æ®
    â””â”€ è¿”å›æœ‰æ•ˆè®ºæ–‡åˆ—è¡¨
```

### 2. ä½œè€…è·å–æµç¨‹

```
get_authors_by_work_ids(work_ids)
    â†“
æå–çŸ­ ID (W123456 from https://openalex.org/W123456)
    â†“
æ ¹æ®æ•°é‡è‡ªé€‚åº”é€‰æ‹© batch_size
    â”œâ”€ â‰¤50 ä¸ª: batch_size = 25-50
    â”œâ”€ â‰¤200 ä¸ª: batch_size = 50
    â”œâ”€ â‰¤500 ä¸ª: batch_size = 60
    â””â”€ >500 ä¸ª: batch_size = 70
    â†“
åˆ†æ‰¹å¤„ç† (æœ€å¤š 100 ä¸ª/æ‰¹)
    â†“
å¯¹æ¯ä¸€æ‰¹ä½¿ç”¨ OR è¯­æ³•æŸ¥è¯¢
    â”œâ”€ filter=openalex:W1|W2|W3|...W50
    â”œâ”€ per_page=50 (å¿…é¡»åŒ¹é…æ‰¹æ¬¡å¤§å°)
    â””â”€ å•ä¸ªè¯·æ±‚è¿”å›æ‰€æœ‰è®ºæ–‡çš„æ‰€æœ‰ä½œè€…
    â†“
å»é‡åˆå¹¶ (ä½¿ç”¨å­—å…¸å»é‡)
    â†“
è¿”å›ä½œè€…åˆ—è¡¨
```

**æ€§èƒ½å¯¹æ¯”**:
- âŒ é€ä¸ªæŸ¥è¯¢: 100 ç¯‡è®ºæ–‡ = 100+ ä¸ªè¯·æ±‚ = ~30 ç§’
- âœ… OR æ‰¹é‡: 100 ç¯‡è®ºæ–‡ = 2 ä¸ªè¯·æ±‚ = ~1 ç§’ (æé€Ÿ 30 å€ï¼)

### 3. åˆä½œå…³ç³»è·å–æµç¨‹

```
get_collaboration_by_authors_batch(author_ids)
    â†“
æå–çŸ­ ID (A123456 from https://openalex.org/A123456)
    â†“
æ ¹æ®ä½œè€…æ•°é‡é€‰æ‹©æœ€ä¼˜ batch_size
    â”œâ”€ è‡ªé€‚åº”ç®—æ³•
    â”œâ”€ å¹³è¡¡è¯·æ±‚æ•°å’Œå•ä¸ªè¯·æ±‚è€—æ—¶
    â””â”€ é€šå¸¸ä¸º 50 ä½ä½œè€…/æ‰¹æ¬¡
    â†“
ä¸¤å±‚å¾ªç¯éå†æ‰¹æ¬¡ (åªæŸ¥è¯¢ i â‰¤ j é¿å…é‡å¤)
    â†“
å¯¹æ¯å¯¹æ‰¹æ¬¡ä½¿ç”¨ OR è¯­æ³•
    â”œâ”€ åŒæ‰¹æ¬¡å†…: filter=author.id:A1|A2|...|A50
    â”‚           (è¿”å›è¿™äº›ä½œè€…çš„æ‰€æœ‰è®ºæ–‡ï¼Œç„¶åç»Ÿè®¡å†…éƒ¨åˆä½œ)
    â”‚
    â””â”€ ä¸åŒæ‰¹æ¬¡: filter=author.id:A1|...|A50,author.id:B1|...|B50
                (è¿”å›åŒæ—¶åŒ…å«ä¸¤ç»„ä½œè€…çš„è®ºæ–‡ï¼Œè¿™äº›å°±æ˜¯åˆä½œè®ºæ–‡)
    â†“
åˆ†é¡µè·å–æ‰€æœ‰è®ºæ–‡ (per_page=200)
    â”œâ”€ ä½¿ç”¨ cursor åˆ†é¡µ
    â”œâ”€ æœ€å¤š max_papers_per_batch=2000 ç¯‡ (é˜²æ­¢è¿‡å¤šåˆ†é¡µ)
    â””â”€ éµå®ˆé€Ÿç‡é™åˆ¶
    â†“
è§£æä½œè€…å…³ç³»
    â”œâ”€ å¯¹æ¯ç¯‡è®ºæ–‡çš„ authorships
    â”œâ”€ æå–ä½œè€…å¯¹ (author_a, author_b)
    â””â”€ ç´¯ç§¯åˆä½œè®¡æ•°
    â†“
è¿”å›åˆä½œå…³ç³»åˆ—è¡¨
```

**æ€§èƒ½å¯¹æ¯”** (ä»¥ 425503 å¯¹ä½œè€…ä¸ºä¾‹):
- âŒ é€å¯¹æŸ¥è¯¢: 425503 å¯¹ = 425503 ä¸ªè¯·æ±‚ = ~95000 ç§’ (26+ å°æ—¶) âŒ
- âœ… æ‰¹é‡æŸ¥è¯¢: 425503 å¯¹ â‰ˆ 100-200 ä¸ªè¯·æ±‚ = ~1000 ç§’ (16-17 åˆ†é’Ÿ) âœ…
- **æ€§èƒ½æå‡: 100-200 å€** ğŸš€

---

## å†…å­˜ç®¡ç†ä¼˜åŒ–

### 1. æµå¼å¤„ç† vs å…¨é‡åŠ è½½

#### âŒ ä¸æ¨è: å…¨é‡åŠ è½½åˆ°å†…å­˜

```python
# é—®é¢˜: ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
all_works = []
for page in pages:
    all_works.extend(fetch_page(page))  # æ‰€æœ‰æ•°æ®åœ¨å†…å­˜ä¸­
# å¤„ç† all_works
```

**é—®é¢˜**:
- å¤§æ•°æ®é‡æ—¶å†…å­˜æº¢å‡º
- 100 ä¸‡æ¡è®ºæ–‡ Ã— 1KB/æ¡ â‰ˆ 1GB å†…å­˜
- GC å‹åŠ›å¤§

#### âœ… æ¨è: æ¸¸æ ‡åˆ†é¡µæµå¼å¤„ç†

```python
# è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ cursor åˆ†é¡µï¼Œåªä¿ç•™å½“å‰é¡µ
cursor = '*'
while cursor and len(all_results) < limit:
    response = fetch_page(cursor)
    results = response['results']
    # å¤„ç†å½“å‰é¡µç»“æœ
    process_results(results)  # åŠæ—¶å¤„ç†ï¼Œé‡Šæ”¾å†…å­˜
    
    cursor = response['meta']['next_cursor']
    all_results.extend(results)  # åªä¿ç•™åˆå¹¶çš„å°‘é‡ç»“æœ
```

**ä¼˜åŠ¿**:
- å†…å­˜å ç”¨æ’å®š (per_page Ã— sizeof(record))
- per_page=200 æ—¶çº¦ 200KB å†…å­˜
- GC å‹åŠ›å°

### 2. æ‰¹å¤„ç†ä¸­çš„å†…å­˜ç®¡ç†

#### batch_size å¯¹å†…å­˜çš„å½±å“

```
batch_size = 20 æ¡è®°å½•
æ¯æ¡è®°å½•å¹³å‡å¤§å° â‰ˆ 10-20 KB (åŒ…å«å…ƒæ•°æ®)

å†…å­˜å ç”¨ â‰ˆ batch_size Ã— å¹³å‡è®°å½•å¤§å°
         = 20 Ã— 15KB = 300KB  âœ… ä½

batch_size = 100 æ¡è®°å½•
å†…å­˜å ç”¨ â‰ˆ 100 Ã— 15KB = 1.5MB  âš ï¸  å¯æ¥å—

batch_size = 500 æ¡è®°å½•
å†…å­˜å ç”¨ â‰ˆ 500 Ã— 15KB = 7.5MB  âŒ è¿‡å¤§
```

#### æ¨èçš„å†…å­˜åˆ†é…ç­–ç•¥

```python
# è®¡ç®—æœ€ä¼˜ batch_size
def calculate_optimal_batch_size(num_items, avg_record_size=15*1024, max_memory=100*1024*1024):
    """
    è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    
    Args:
        num_items: è¦å¤„ç†çš„æ€»é¡¹ç›®æ•°
        avg_record_size: å¹³å‡è®°å½•å¤§å° (å­—èŠ‚)
        max_memory: å…è®¸çš„æœ€å¤§å†…å­˜ (å­—èŠ‚)
    
    Returns:
        æœ€ä¼˜ batch_size
    """
    max_batch_size = max_memory // avg_record_size
    
    if num_items <= 50:
        return num_items if num_items <= 25 else 25
    elif num_items <= 200:
        return min(50, max_batch_size)
    elif num_items <= 500:
        return min(60, max_batch_size)
    else:
        return min(70, max_batch_size)
```

### 3. å»é‡æ•°æ®ç»“æ„ä¼˜åŒ–

#### âŒ ä½æ•ˆ: ä½¿ç”¨åˆ—è¡¨+çº¿æ€§æŸ¥æ‰¾

```python
authors = []
for work in works:
    for authorship in work['authorships']:
        author = authorship['author']
        if author not in authors:  # O(n) æŸ¥æ‰¾
            authors.append(author)
# æ—¶é—´å¤æ‚åº¦: O(nÂ²)
```

#### âœ… é«˜æ•ˆ: ä½¿ç”¨å­—å…¸/Set

```python
authors = {}  # ä½¿ç”¨ dict ä½œä¸º set
for work in works:
    for authorship in work['authorships']:
        author = authorship['author']
        author_id = author['id']
        if author_id not in authors:  # O(1) æŸ¥æ‰¾
            authors[author_id] = author
# æ—¶é—´å¤æ‚åº¦: O(n)
return list(authors.values())
```

**æ€§èƒ½å¯¹æ¯”** (1000 ä½ä½œè€…):
- åˆ—è¡¨æ–¹æ¡ˆ: 1000Â² Ã· 2 â‰ˆ 500k æ¬¡æ¯”è¾ƒ
- å­—å…¸æ–¹æ¡ˆ: 1000 æ¬¡ O(1) æŸ¥æ‰¾

### 4. ç¼“å­˜å¯¹å†…å­˜çš„å½±å“

```python
# Redis ç¼“å­˜: å­˜å‚¨åœ¨å¤–éƒ¨è¿›ç¨‹ï¼Œä¸å ç”¨åº”ç”¨å†…å­˜
cache.set(key, data, timeout=86400)  # 24å°æ—¶

# å†…å­˜ç¼“å­˜: ç›´æ¥å ç”¨åº”ç”¨è¿›ç¨‹å†…å­˜
memory_cache[key] = data

# æ··åˆç­–ç•¥: çƒ­æ•°æ®åœ¨å†…å­˜ï¼Œå†·æ•°æ®åœ¨ Redis
if key in memory_cache:
    return memory_cache[key]  # å¿«é€Ÿ
elif key in redis_cache:
    data = redis_cache.get(key)
    memory_cache[key] = data  # å‡çº§åˆ°å†…å­˜
    return data
else:
    data = fetch_from_api()  # ä» API è·å–
    memory_cache[key] = data
    redis_cache.set(key, data)
    return data
```

---

## å¹¶å‘æ§åˆ¶æœºåˆ¶

### 1. å•çº¿ç¨‹ vs å¤šçº¿ç¨‹ vs å¼‚æ­¥

#### OpenAlex API çš„ç‰¹æ®Šé™åˆ¶

âš ï¸ **å…³é”®é™åˆ¶**: OpenAlex çš„ cursor åˆ†é¡µä¸æ”¯æŒçœŸæ­£çš„å¹¶å‘ï¼

```
é—®é¢˜: å¦‚æœä¸¤ä¸ªçº¿ç¨‹åŒæ—¶ä½¿ç”¨ä¸åŒçš„ cursorï¼Œä¼šè¿”å›é‡å æ•°æ®

ç¤ºä¾‹:
Thread 1: fetch(cursor='A') â†’ results [1-200]
Thread 2: fetch(cursor='B') â†’ results [150-350]  âŒ é‡å  [150-200]
```

### 2. å½“å‰å®ç°: é¡ºåºå¤„ç† + é€Ÿç‡é™åˆ¶

```python
# app/data/openalex_fetcher.py - RateLimiter ç±»

class RateLimiter:
    """ä»¤ç‰Œæ¡¶é™æµå™¨ - ç²¾ç¡®æ§åˆ¶é€Ÿç‡"""
    
    def __init__(self, max_requests_per_second=10):
        self.max_rps = 10  # OpenAlex å®˜æ–¹é™åˆ¶
        self.min_interval = 1.0 / 10  # 0.1 ç§’
        self.last_request_time = 0
        self.lock = Lock()  # å¤šçº¿ç¨‹å®‰å…¨
    
    def acquire(self, timeout=60):
        """è·å–è®¸å¯è¯ - é˜»å¡ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€"""
        with self.lock:
            now = time.time()
            time_since_last = now - self.last_request_time
            
            if time_since_last < self.min_interval:
                wait_time = self.min_interval - time_since_last
                time.sleep(wait_time)  # ç­‰å¾…
            
            self.last_request_time = time.time()
            return True
```

**å·¥ä½œåŸç†**:

```
æ—¶é—´è½´:
0.0s: è¯·æ±‚1 å‘å‡º â”€â”€â”€â”€â”€â”€â”€â”€â–º 0.1s: è¯·æ±‚2 å¯ä»¥å‘å‡º
0.1s: è¯·æ±‚2 å‘å‡º â”€â”€â”€â”€â”€â”€â”€â”€â–º 0.2s: è¯·æ±‚3 å¯ä»¥å‘å‡º
0.2s: è¯·æ±‚3 å‘å‡º â”€â”€â”€â”€â”€â”€â”€â”€â–º 0.3s: è¯·æ±‚4 å¯ä»¥å‘å‡º
...

ä¿è¯: ç›¸é‚»è¯·æ±‚é—´éš” â‰¥ 0.1s
     æ€»é€Ÿç‡ â‰¤ 10 req/s
```

### 3. çº¿ç¨‹é”ä¿æŠ¤å…±äº«èµ„æº

```python
# å¤šçº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶å™¨

self.lock = Lock()  # äº’æ–¥é”

def acquire(self):
    with self.lock:  # è·å–é”
        # ä¸´ç•ŒåŒº - åŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œ
        now = time.time()
        time_since_last = now - self.last_request_time
        
        if time_since_last < self.min_interval:
            wait_time = self.min_interval - time_since_last
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    # é‡Šæ”¾é”
    return True
```

**ç«æ€æ¡ä»¶ç¤ºä¾‹** (æ²¡æœ‰é”æ—¶):

```
âŒ é—®é¢˜:

Thread 1: read(last_request_time=0)
Thread 2: read(last_request_time=0)  âŒ åŒæ—¶è¯»åˆ° 0ï¼
Thread 1: write(last_request_time=0.1)
Thread 2: write(last_request_time=0.1)

ç»“æœ: ä¸¤ä¸ªè¯·æ±‚åœ¨ 0.1s å†…éƒ½å‘å‡ºï¼Œè¿åé€Ÿç‡é™åˆ¶

âœ… ä½¿ç”¨é”è§£å†³:

Thread 1: acquire lock
Thread 1: read(last_request_time=0)
Thread 1: write(last_request_time=0.1)
Thread 1: release lock
       â†“ (ç­‰å¾…ä¸­)
Thread 2: acquire lock  
Thread 2: read(last_request_time=0.1)
Thread 2: sleep(0.09s)  # ç­‰å¾…åˆ° 0.19s
Thread 2: write(last_request_time=0.19)
Thread 2: release lock

ç»“æœ: ä¸¤ä¸ªè¯·æ±‚é—´éš” â‰¥ 0.1s âœ…
```

### 4. ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ ThreadPoolExecutor

è™½ç„¶ä»£ç ä¸­å¯¼å…¥äº† `ThreadPoolExecutor`:

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
```

ä½†å®é™…ä¸Š **æ²¡æœ‰åœ¨ä¸»æµç¨‹ä¸­ä½¿ç”¨** å¤šçº¿ç¨‹å¹¶å‘ï¼ŒåŸå› æ˜¯:

1. **OpenAlex API é™åˆ¶**: cursor åˆ†é¡µä¸æ”¯æŒå¹¶å‘
2. **é€Ÿç‡é™åˆ¶**: 10 req/sï¼Œå¤šçº¿ç¨‹æ— æ³•åŠ é€Ÿ
3. **ç®€æ´æ€§**: é¡ºåºå¤„ç†æ›´å®¹æ˜“è°ƒè¯•å’Œç»´æŠ¤

**ä½•æ—¶ä½¿ç”¨å¤šçº¿ç¨‹**:

```python
# é€‚ç”¨åœºæ™¯: å¤„ç†å¤šä¸ªç‹¬ç«‹çš„ä»»åŠ¡
# ä¾‹å¦‚: åŒæ—¶å¤„ç† 10 ä¸ªä¸åŒå­¦ç§‘çš„è®ºæ–‡æœç´¢

from concurrent.futures import ThreadPoolExecutor

def search_multiple_disciplines(disciplines):
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(search_works, d): d 
            for d in disciplines
        }
        
        results = {}
        for future in as_completed(futures):
            discipline = futures[future]
            results[discipline] = future.result()
    
    return results

# æ³¨æ„: 3 workers + 10 req/s = é€Ÿç‡é™åˆ¶ä»ç„¶é€‚ç”¨
#      éœ€è¦ç¡®ä¿æ€»é€Ÿç‡ â‰¤ 10 req/s
```

### 5. Session è¿æ¥æ± ä¼˜åŒ–

```python
# openalex_fetcher.py - è¿æ¥å¤ç”¨

self.session = requests.Session()
self.session.headers.update({
    'User-Agent': f'OpenAlexBackend (mailto:{self.email})',
})

# ä½¿ç”¨ Session çš„ä¼˜åŠ¿:
# 1. TCP è¿æ¥å¤ç”¨ (é¿å… 3-way handshake å¼€é”€)
# 2. HTTP Keep-Alive (å‡å°‘å»¶è¿Ÿ)
# 3. è‡ªåŠ¨ DNS ç¼“å­˜
# 4. Cookie ç®¡ç†

# æ€§èƒ½å¯¹æ¯”:
# ä¸ä½¿ç”¨ Session: 1000 è¯·æ±‚ Ã— (10ms TCP + 50ms HTTP) = 60s
# ä½¿ç”¨ Session: 1000 è¯·æ±‚ Ã— (1ms TCP + 50ms HTTP) = 51s
#              æˆ–è€…: 1000 è¯·æ±‚ Ã— 50ms = 50s (TCP å¤ç”¨)
```

---

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. æ ¸å¿ƒä¼˜åŒ–: OR è¯­æ³•æ‰¹é‡æŸ¥è¯¢

#### é—®é¢˜èƒŒæ™¯

OpenAlex API æ”¯æŒ OR è¯­æ³•åœ¨å•ä¸ªè¯·æ±‚ä¸­æŸ¥è¯¢å¤šä¸ªå€¼:

```
å•ä¸ªæŸ¥è¯¢:
GET /works?filter=openalex:W123456

å¤šä¸ªæŸ¥è¯¢ (é€ä¸ª):
GET /works?filter=openalex:W123456
GET /works?filter=openalex:W234567
GET /works?filter=openalex:W345678
...

OR æ‰¹é‡æŸ¥è¯¢ (æ¨è):
GET /works?filter=openalex:W123456|W234567|W345678|...
```

#### å®ç°ç»†èŠ‚

```python
def get_authors_by_work_ids(work_ids):
    """è·å–è®ºæ–‡çš„ä½œè€… - OR è¯­æ³•ä¼˜åŒ–"""
    
    authors = {}
    
    # åˆ†æ‰¹å¤„ç†
    batch_size = 20  # æ¯æ‰¹ 20 ç¯‡è®ºæ–‡
    batches = batch_by_or_syntax(work_ids, batch_size)
    
    for batch in batches:
        # æ„å»º OR è¿‡æ»¤å™¨
        or_filter = '|'.join(batch)  # W1|W2|W3|...
        
        params = {
            'filter': f'openalex:{or_filter}',
            'per_page': len(batch),  # å¿…é¡»åŒ¹é…æ‰¹æ¬¡å¤§å°ï¼
        }
        
        response = make_request('/works', params)
        works = response['results']
        
        # æå–ä½œè€…
        for work in works:
            for authorship in work['authorships']:
                author_id = authorship['author']['id']
                if author_id not in authors:
                    authors[author_id] = authorship['author']
    
    return list(authors.values())
```

**å…³é”®å‚æ•°**:

| å‚æ•° | å«ä¹‰ | é™åˆ¶ |
|------|------|------|
| `batch_size` | å•ä¸ªè¯·æ±‚ä¸­çš„é¡¹ç›®æ•° | â‰¤ 100 (å®˜æ–¹é™åˆ¶) |
| `per_page` | æ¯é¡µè¿”å›æ•°é‡ | å¿…é¡» â‰¥ batch_size |
| `filter` | OR è¯­æ³•è¿‡æ»¤ | `id1\|id2\|id3` |

#### æ€§èƒ½å¯¹æ¯” (100 ç¯‡è®ºæ–‡)

```
æ–¹æ¡ˆ 1: é€ä¸ªæŸ¥è¯¢
- è¯·æ±‚æ•°: 100
- æ¯ä¸ªè¯·æ±‚è€—æ—¶: 500ms (API + ç½‘ç»œ)
- æ€»è€—æ—¶: 100 Ã— 500ms = 50s

æ–¹æ¡ˆ 2: æ‰¹é‡æŸ¥è¯¢ (batch_size=20)
- è¯·æ±‚æ•°: 5
- æ¯ä¸ªè¯·æ±‚è€—æ—¶: 600ms (æ›´å¤šæ•°æ®ï¼Œä½†çœå»ç½‘ç»œå¾€è¿”)
- æ€»è€—æ—¶: 5 Ã— 600ms = 3s

æ€§èƒ½æå‡: 50s Ã· 3s â‰ˆ 17 å€ ğŸš€
```

### 2. è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°

#### é—®é¢˜

å›ºå®šçš„ batch_size ä¸æ˜¯æœ€ä¼˜çš„:

- æ•°æ®å°‘æ—¶ (10 æ¡): batch_size=50 æµªè´¹äº†
- æ•°æ®å¤šæ—¶ (1000 æ¡): batch_size=50 éœ€è¦ 20 ä¸ªè¯·æ±‚

#### è§£å†³æ–¹æ¡ˆ

```python
def _get_optimal_batch_size(num_items):
    """æ ¹æ®æ•°æ®é‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ batch_size"""
    
    if num_items <= 50:
        # å°è§„æ¨¡: ä¸€ä¸ªè¯·æ±‚è¶³å¤Ÿ
        return num_items if num_items <= 25 else 25
    
    elif num_items <= 200:
        # ä¸­ç­‰è§„æ¨¡: batch_size=50 æ˜¯å¹³è¡¡ç‚¹
        # æ€§èƒ½å¯¹æ¯”:
        # batch_size=30: 7 ä¸ªè¯·æ±‚ Ã— 0.5s = 3.5s
        # batch_size=50: 4 ä¸ªè¯·æ±‚ Ã— 0.7s = 2.8s âœ… æœ€ä¼˜
        # batch_size=70: 3 ä¸ªè¯·æ±‚ Ã— 1.0s = 3.0s
        return 50
    
    elif num_items <= 500:
        # å¤§è§„æ¨¡: batch_size=60
        return 60
    
    else:
        # è¶…å¤§è§„æ¨¡: batch_size=70
        return 70
```

**æ€§èƒ½ç†è®º**:

```
æ€»è€—æ—¶ â‰ˆ (num_items / batch_size) Ã— request_overhead + single_request_time

request_overhead â‰ˆ 100ms (ç½‘ç»œå¾€è¿”)
single_request_time â‰ˆ 200ms + batch_size Ã— 5ms

ä»¥ 200 é¡¹ä¸ºä¾‹:
batch_size=30: (200/30) Ã— 100 + 200 + 30Ã—5 = 667 + 200 + 150 = 1017ms
batch_size=50: (200/50) Ã— 100 + 200 + 50Ã—5 = 400 + 200 + 250 = 850ms âœ…
batch_size=70: (200/70) Ã— 100 + 200 + 70Ã—5 = 286 + 200 + 350 = 836ms (æ¥è¿‘)
```

### 3. ä¸¤å±‚å¾ªç¯ OR æŸ¥è¯¢ (åˆä½œå…³ç³»)

#### é—®é¢˜

è®¡ç®— N ä½ä½œè€…çš„ä¸¤ä¸¤åˆä½œå…³ç³»:

```
ç›´æ¥æ–¹æ³•: æšä¸¾æ‰€æœ‰å¯¹
for i in range(N):
    for j in range(i+1, N):
        query(author_i, author_j)  # é€å¯¹æŸ¥è¯¢

è¯·æ±‚æ•° = N Ã— (N-1) / 2
ä¾‹: 425503 ä½ä½œè€… = 90 äº¿å¯¹ï¼âŒ
```

#### è§£å†³æ–¹æ¡ˆ: åˆ†ç»„ OR æŸ¥è¯¢

```python
def get_collaboration_by_authors_batch(author_ids):
    """ä½¿ç”¨ OR æ‰¹é‡æŸ¥è¯¢åˆä½œå…³ç³»"""
    
    # 1. åˆ†ç»„
    batch_size = 50  # è‡ªé€‚åº”é€‰æ‹©
    batches = batch_by_or_syntax(author_ids, batch_size)
    # ç»“æœ: [A1-A50, A51-A100, ..., A451-A503]
    
    # 2. ä¸¤å±‚å¾ªç¯ (åªæŸ¥è¯¢ i â‰¤ j)
    collaborations = {}
    
    for i, batch_a in enumerate(batches):
        for j, batch_b in enumerate(batches):
            if i > j:
                continue  # é¿å…é‡å¤æŸ¥è¯¢
            
            # 3. ä½¿ç”¨ OR è¯­æ³•æŸ¥è¯¢
            group_a = '|'.join(batch_a)      # A1|A2|...|A50
            group_b = '|'.join(batch_b)      # A51|A52|...|A100
            
            filter_str = f'author.id:{group_a},author.id:{group_b}'
            # è¿™è¡¨ç¤º: (A1 OR A2 OR ... OR A50) AND (A51 OR A52 OR ... OR A100)
            
            # 4. è·å–è®ºæ–‡å¹¶ç»Ÿè®¡
            params = {
                'filter': filter_str,
                'per_page': 200,
            }
            
            response = make_request('/works', params)
            works = response['results']
            
            # 5. è§£æåˆä½œå…³ç³»
            for work in works:
                for author_a in get_authors(work, batch_a):
                    for author_b in get_authors(work, batch_b):
                        if i != j or author_a < author_b:
                            pair = (author_a, author_b)
                            collaborations[pair] = collaborations.get(pair, 0) + 1

    return collaborations
```

**è¯·æ±‚æ•°è®¡ç®—**:

```
åˆ†æ‰¹æ•° = ceil(N / batch_size)
ä¾‹: 425503 / 50 = 8511 æ‰¹æ¬¡

è¯·æ±‚æ•° = batches Ã— (batches + 1) / 2
ä¾‹: 8511 Ã— 8512 / 2 â‰ˆ 36 ç™¾ä¸‡ âŒ

ç­‰ç­‰ï¼Œè¿™è¿˜æ˜¯å¤ªå¤šäº†ï¼éœ€è¦åˆ†é¡µ...

å®é™…æƒ…å†µ:
- æ¯ä¸ªè¿‡æ»¤æŸ¥è¯¢è¿”å› ~2000 ç¯‡è®ºæ–‡
- éœ€è¦åˆ†é¡µè·å–: 2000 / 200 = 10 é¡µ
- æ€»è¯·æ±‚æ•° â‰ˆ 36M / 2000 = 1.8M âŒ è¿˜æ˜¯å¤ªå¤š

æ­£ç¡®çš„è®¡ç®—:
- cursor åˆ†é¡µ: æ¯æ‰¹æŸ¥è¯¢ä¼šæœ‰å¤šä¸ª cursor è¯·æ±‚
- å®é™…è¯·æ±‚æ•° â‰ˆ 100-200 (å•çº¿ç¨‹, per_page=200, max_papers=2000)

åŸå› : max_papers_per_batch=2000 é™åˆ¶äº†æ¯ä¸ª (batch_a, batch_b) ç»„åˆ
      æœ€å¤šè·å– 2000 ç¯‡è®ºæ–‡ï¼Œå³ 10 é¡µï¼Œæ—©æœŸåœæ­¢
```

### 4. é¡µé¢å¤§å°ä¼˜åŒ– (per_page)

```python
# å®˜æ–¹é™åˆ¶: per_page æœ€å¤§ä¸º 200
per_page = 200  # æœ€å¤§å€¼

# æ€§èƒ½å¯¹æ¯”:
# per_page=50: 10 é¡µè¯·æ±‚ = 1000ms
# per_page=100: 5 é¡µè¯·æ±‚ = 500ms
# per_page=200: 2-3 é¡µè¯·æ±‚ = 300ms âœ…

# æƒè¡¡:
# æ›´å¤§çš„ per_page:
# - ä¼˜ç‚¹: æ›´å°‘çš„ç½‘ç»œå¾€è¿”
# - ç¼ºç‚¹: å•ä¸ªè¯·æ±‚æ›´æ…¢ï¼Œå®¹æ˜“è¶…æ—¶
# 200 æ˜¯æœ€ä¼˜å¹³è¡¡ç‚¹
```

### 5. è¶…æ—¶å’Œé‡è¯•ç­–ç•¥

```python
def _make_request(endpoint, params, timeout=60, max_retries=3):
    """å¸¦é‡è¯•çš„ API è¯·æ±‚"""
    
    for attempt in range(max_retries + 1):
        try:
            response = session.get(url, params=params, timeout=timeout)
            
            if response.status_code == 429:  # é€Ÿç‡é™åˆ¶
                if attempt < max_retries:
                    wait_time = 2 ** (attempt + 1)  # æŒ‡æ•°é€€é¿
                    time.sleep(wait_time)
                    continue
            
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            if attempt < max_retries:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
            raise
```

**æŒ‡æ•°é€€é¿é€»è¾‘**:

```
è¯·æ±‚ 1: å¤±è´¥ â†’ ç­‰å¾… 2s
è¯·æ±‚ 2: å¤±è´¥ â†’ ç­‰å¾… 4s
è¯·æ±‚ 3: å¤±è´¥ â†’ ç­‰å¾… 8s
è¯·æ±‚ 4: å¤±è´¥ â†’ æ”¾å¼ƒ (max_retries=3)

ä¼˜åŠ¿:
- çŸ­æ—¶æ•…éšœè‡ªåŠ¨æ¢å¤
- ä¸ä¼šç«‹å³é‡è¯•å¯¼è‡´æ›´å¤š 429 é”™è¯¯
- ç»™æœåŠ¡å™¨æ¢å¤æ—¶é—´
```

---

## API é€Ÿç‡é™åˆ¶

### OpenAlex é€Ÿç‡é™åˆ¶ç­–ç•¥

#### 1. å®˜æ–¹é™åˆ¶

```
å…è´¹è®¡åˆ’ (Polite Pool):
- 10 è¯·æ±‚/ç§’
- 100,000 è¯·æ±‚/å¤©
- å“åº”æ—¶é—´: 5-30 ç§’

Premium è®¡åˆ’:
- 10+ è¯·æ±‚/ç§’ (å–å†³äºè®¢é˜…çº§åˆ«)
- æ— æ—¥é™é¢
- æ›´ç¨³å®šçš„å“åº”æ—¶é—´
```

#### 2. åŠ å…¥ Polite Pool

```python
# æ·»åŠ  mailto å‚æ•°
params = {
    'mailto': 'your-email@example.com',  # âœ… åŠ å…¥ Polite Pool
    'per_page': 200,
    'filter': '...',
}

response = requests.get('https://api.openalex.org/works', params=params)
```

**Polite Pool ä¼˜åŠ¿**:
- å“åº”æ—¶é—´æ›´ç¨³å®š
- æ›´æœ‰å¯èƒ½è·å¾—æ›´é«˜çš„é€Ÿç‡é™åˆ¶
- å…è´¹åŠ å…¥ (åªéœ€æä¾›é‚®ç®±)

#### 3. å½“å‰å®ç°: ä»¤ç‰Œæ¡¶é™æµ

```python
class RateLimiter:
    def __init__(self, max_requests_per_second=10):
        self.max_rps = 10
        self.min_interval = 1.0 / 10  # 0.1 ç§’
        self.last_request_time = 0
        self.lock = Lock()
    
    def acquire(self, timeout=60):
        """è·å–å‘é€è®¸å¯è¯"""
        with self.lock:
            now = time.time()
            time_since_last = now - self.last_request_time
            
            if time_since_last < self.min_interval:
                wait_time = self.min_interval - time_since_last
                time.sleep(wait_time)
            
            self.last_request_time = time.time()
            return True

# ä½¿ç”¨
rate_limiter = RateLimiter(max_requests_per_second=10)

def _rate_limit(self):
    success = self.rate_limiter.acquire(timeout=60)
    if not success:
        logger.warning("âš ï¸ é€Ÿç‡é™åˆ¶è¶…æ—¶")

def _make_request(self, endpoint, params):
    self._rate_limit()  # ä¿è¯é€Ÿç‡
    response = self.session.get(url, params=params)
```

**å·¥ä½œåŸç†**:

```
ä»¤ç‰Œæ¡¶ç®—æ³•:
- æ¯ 0.1s äº§ç”Ÿ 1 ä¸ªä»¤ç‰Œ (10 req/s)
- å‘é€è¯·æ±‚å‰å¿…é¡»è·å¾—ä»¤ç‰Œ
- æ²¡æœ‰ä»¤ç‰Œæ—¶é˜»å¡ç­‰å¾…

æ—¶é—´çº¿:
t=0.0s: è¯·æ±‚1 æ¶ˆè€—ä»¤ç‰Œ (æ— éœ€ç­‰å¾…)
t=0.0s-0.1s: è¯·æ±‚2 ç­‰å¾…ä»¤ç‰Œ
t=0.1s: è¯·æ±‚2 æ¶ˆè€—ä»¤ç‰Œ
t=0.1s-0.2s: è¯·æ±‚3 ç­‰å¾…ä»¤ç‰Œ
t=0.2s: è¯·æ±‚3 æ¶ˆè€—ä»¤ç‰Œ

ç»“æœ: æ¯ 0.1s ä¸€ä¸ªè¯·æ±‚ â‰ˆ 10 req/s âœ…
```

---

## æ‰¹é‡å¤„ç†ä¼˜åŒ–

### 1. æ‰¹é‡è·å–è®ºæ–‡ä½œè€…

```python
# åœºæ™¯: è·å– 1000 ç¯‡è®ºæ–‡çš„æ‰€æœ‰ä½œè€…

# âŒ ä½æ•ˆ: é€ç¯‡è·å–
authors_all = []
for work_id in work_ids:
    work = get_work_by_id(work_id)
    authors_all.extend(work['authorships'])
# 1000 ä¸ªè¯·æ±‚ Ã— 500ms = 500s âŒ

# âœ… é«˜æ•ˆ: æ‰¹é‡ OR æŸ¥è¯¢
authors = get_authors_by_work_ids(work_ids)
# 50 ä¸ªè¯·æ±‚ Ã— 600ms = 30s âœ…
# æ€§èƒ½æå‡: 500s / 30s â‰ˆ 17 å€
```

### 2. æ‰¹é‡è·å–åˆä½œå…³ç³»

```python
# åœºæ™¯: è·å– 500 ä½ä½œè€…çš„åˆä½œå…³ç³»çŸ©é˜µ

# âŒ æå…¶ä½æ•ˆ: é€å¯¹æŸ¥è¯¢
for i, author_a in enumerate(authors):
    for j, author_b in enumerate(authors[i+1:]):
        collaborations.append({
            'from': author_a,
            'to': author_b,
            'count': get_collaboration_count(author_a, author_b)
        })
# 500 Ã— 499 / 2 = 124750 å¯¹
# Ã— 500ms/å¯¹ = 62 ç™¾ä¸‡ç§’ = 718 å¤© âŒâŒâŒ

# âœ… é«˜æ•ˆ: æ‰¹é‡ OR æŸ¥è¯¢
collaborations = get_collaboration_by_authors_batch(authors)
# ~100-200 ä¸ªè¯·æ±‚ = 1000s = 16 åˆ†é’Ÿ âœ…
# æ€§èƒ½æå‡: 718 å¤© / 16 åˆ†é’Ÿ = 63 ä¸‡å€ï¼ï¼ï¼ ğŸš€ğŸš€ğŸš€
```

### 3. å‚æ•°éªŒè¯æ‰¹å¤„ç†

```python
class OpenAlexParamValidator:
    """å‚æ•°éªŒè¯ - æ”¯æŒæ‰¹é‡è½¬æ¢"""
    
    # å­¦ç§‘ ID æ˜ å°„
    DISCIPLINE_ALIASES = {
        'cs': 'T13674',
        'machine_learning': 'T10470',
        'deep_learning': 'T12600',
        # ... 100+ æ˜ å°„
    }
    
    @staticmethod
    def validate_and_convert_disciplines(discipline_str):
        """æ‰¹é‡éªŒè¯å­¦ç§‘
        
        è¾“å…¥: "CS,Machine Learning,Deep Learning"
        è¾“å‡º: ['T13674', 'T10470', 'T12600']
        """
        if not discipline_str:
            return []
        
        disciplines = [d.strip().lower() for d in discipline_str.split(',')]
        validated = []
        
        for d in disciplines:
            if d in OpenAlexParamValidator.DISCIPLINE_ALIASES:
                validated.append(OpenAlexParamValidator.DISCIPLINE_ALIASES[d])
        
        return validated
```

---

## ç¼“å­˜æœºåˆ¶

### 1. ç¼“å­˜å±‚æ¬¡

```
ç¬¬ 1 å±‚: å†…å­˜ç¼“å­˜ (L1)
â”œâ”€ é€Ÿåº¦: <1ms
â”œâ”€ å®¹é‡: ~100MB (å–å†³äºåº”ç”¨å†…å­˜)
â”œâ”€ ä½¿ç”¨: çƒ­æ•°æ®
â””â”€ å®ç°: Python dict

ç¬¬ 2 å±‚: Redis ç¼“å­˜ (L2)
â”œâ”€ é€Ÿåº¦: 1-10ms (ç½‘ç»œå»¶è¿Ÿ)
â”œâ”€ å®¹é‡: ~1GB-10GB
â”œâ”€ ä½¿ç”¨: æ¸©æ•°æ®
â””â”€ å®ç°: Redis client

ç¬¬ 3 å±‚: API æ•°æ®æº
â”œâ”€ é€Ÿåº¦: 500ms-2s (ç½‘ç»œ + è®¡ç®—)
â”œâ”€ å®¹é‡: æ— é™
â””â”€ ä½¿ç”¨: å†·æ•°æ®
```

### 2. ç¼“å­˜ Key è®¾è®¡

```python
# è®ºæ–‡æ•°æ®
cache_key = f"paper:{paper_id}"
cache_key = f"papers:year:{2020}:limit:{200}"

# ä½œè€…æ•°æ®
cache_key = f"author:{author_id}"
cache_key = f"authors:work_ids:{len(work_ids)}:{limit}"

# åˆä½œå…³ç³»
cache_key = f"collaborations:author_count:{len(author_ids)}"

# ç»Ÿè®¡æ•°æ®
cache_key = f"stats:year:{year}:discipline:{discipline}"
```

**Key å‘½åè§„èŒƒ**:
- ä½¿ç”¨ `:` åˆ†éš”å±‚çº§
- åŒ…å«å½±å“ç»“æœçš„å‚æ•°
- æ˜“äº debug å’Œç®¡ç†

### 3. ç¼“å­˜å¤±æ•ˆç­–ç•¥

```python
class DataCache:
    def __init__(self, use_redis=False):
        self.memory_cache = {}
        self.use_redis = use_redis
        self.ttl = 86400  # 24 å°æ—¶
    
    def get(self, key):
        """è·å–ç¼“å­˜ - å…ˆæŸ¥å†…å­˜ï¼Œå†æŸ¥ Redis"""
        
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if key in self.memory_cache:
            data, expiry = self.memory_cache[key]
            if time.time() < expiry:
                return data  # å‘½ä¸­ï¼Œä¸”æœªè¿‡æœŸ
            else:
                del self.memory_cache[key]  # è¿‡æœŸï¼Œåˆ é™¤
        
        # æ£€æŸ¥ Redis ç¼“å­˜
        if self.use_redis:
            data = self.redis_client.get(key)
            if data:
                # å‡çº§åˆ°å†…å­˜ç¼“å­˜
                self.memory_cache[key] = (data, time.time() + self.ttl)
                return data
        
        return None  # ç¼“å­˜æœªå‘½ä¸­
    
    def set(self, key, value, ttl=None):
        """å­˜å‚¨ç¼“å­˜"""
        ttl = ttl or self.ttl
        
        # å­˜å‚¨åˆ°å†…å­˜
        self.memory_cache[key] = (value, time.time() + ttl)
        
        # å­˜å‚¨åˆ° Redis
        if self.use_redis:
            self.redis_client.setex(key, ttl, json.dumps(value))
    
    def invalidate(self, pattern):
        """ä½¿ç”¨æ¨¡å¼å¤±æ•ˆç¼“å­˜
        
        ä¾‹: invalidate('papers:year:2020:*')
        """
        # æ¸…ç©ºå†…å­˜ç¼“å­˜ä¸­åŒ¹é…çš„é¡¹
        keys_to_delete = [k for k in self.memory_cache.keys() if match_pattern(k, pattern)]
        for k in keys_to_delete:
            del self.memory_cache[k]
        
        # æ¸…ç©º Redis ç¼“å­˜ä¸­åŒ¹é…çš„é¡¹
        if self.use_redis:
            self.redis_client.delete(*keys_to_delete)
```

### 4. ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–

```python
# è¿½è¸ªç¼“å­˜ç»Ÿè®¡
class CacheStats:
    def __init__(self):
        self.hits = 0
        self.misses = 0
    
    def hit(self):
        self.hits += 1
    
    def miss(self):
        self.misses += 1
    
    @property
    def hit_rate(self):
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

# ä½¿ç”¨
cache_stats = CacheStats()

def get_data(key):
    if key in cache:
        cache_stats.hit()
        return cache[key]
    else:
        cache_stats.miss()
        data = fetch_from_api(key)
        cache[key] = data
        return data

# ç›‘æ§
logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.hit_rate:.1%}")
```

**ç›®æ ‡**:
- çƒ­ API: >80% å‘½ä¸­ç‡
- æ¸© API: >50% å‘½ä¸­ç‡
- å†· API: >10% å‘½ä¸­ç‡

---

## æ•…éšœå¤„ç†ä¸é‡è¯•

### 1. å¼‚å¸¸ç±»å‹

```python
# API å¼‚å¸¸
requests.exceptions.Timeout  # è¯·æ±‚è¶…æ—¶
requests.exceptions.ConnectionError  # è¿æ¥é”™è¯¯
requests.exceptions.HTTPError  # HTTP é”™è¯¯ (4xx, 5xx)

# OpenAlex ç‰¹å®šå¼‚å¸¸
429  # Rate Limit (é€Ÿç‡é™åˆ¶)
404  # Not Found (æ•°æ®ä¸å­˜åœ¨)
400  # Bad Request (å‚æ•°é”™è¯¯)
500  # Server Error (æœåŠ¡å™¨é”™è¯¯)
```

### 2. é‡è¯•ç­–ç•¥

```python
def _make_request(endpoint, params, max_retries=3):
    """å¸¦é‡è¯•çš„ API è¯·æ±‚"""
    
    for attempt in range(max_retries + 1):
        try:
            response = session.get(url, params=params, timeout=60)
            
            # å¤„ç†é€Ÿç‡é™åˆ¶
            if response.status_code == 429:
                if attempt < max_retries:
                    wait_time = 2 ** (attempt + 1)  # 2s, 4s, 8s
                    logger.warning(f"è§¦å‘é€Ÿç‡é™åˆ¶ï¼Œ{wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    response.raise_for_status()
            
            response.raise_for_status()
            return response.json()
        
        except requests.exceptions.Timeout:
            logger.error(f"è¯·æ±‚è¶…æ—¶ (attempt {attempt + 1}/{max_retries + 1})")
            if attempt < max_retries:
                time.sleep(2 ** attempt)  # 1s, 2s, 4s
                continue
            raise
        
        except requests.exceptions.ConnectionError:
            logger.error(f"è¿æ¥é”™è¯¯ (attempt {attempt + 1}/{max_retries + 1})")
            if attempt < max_retries:
                time.sleep(5)  # å›ºå®š 5s ç­‰å¾…
                continue
            raise
```

**é‡è¯•ç®—æ³•**:

```
Attempt 1: å³æ—¶é‡è¯• (0s ç­‰å¾…)
Attempt 2: ç­‰å¾… 2s (2^1)
Attempt 3: ç­‰å¾… 4s (2^2)
Attempt 4: ç­‰å¾… 8s (2^3)
Attempt 5: æ”¾å¼ƒ âŒ

æ€»è€—æ—¶: 0 + 2 + 4 + 8 = 14s
å®¹é”™: æœ€å¤š 14s çš„å»¶è¿Ÿï¼Œé¿å…æœåŠ¡é›ªå´©
```

### 3. éƒ¨åˆ†æˆåŠŸå¤„ç†

```python
def get_collaboration_by_authors_batch(author_ids):
    """å³ä½¿éƒ¨åˆ†æ‰¹æ¬¡å¤±è´¥ï¼Œä¹Ÿè¿”å›å·²è·å–çš„ç»“æœ"""
    
    collaborations = {}
    failed_batches = []
    
    for batch_a, batch_b in batch_pairs:
        try:
            works = query_collaboration(batch_a, batch_b)
            collaborations.update(parse_works(works))
        
        except Exception as e:
            logger.warning(f"æ‰¹æ¬¡å¤±è´¥: {batch_a} Ã— {batch_b}, é”™è¯¯: {e}")
            failed_batches.append((batch_a, batch_b))
            continue  # ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡
    
    logger.info(f"å®Œæˆ: {len(collaborations)} å¯¹åˆä½œ, å¤±è´¥: {len(failed_batches)} æ‰¹")
    return collaborations, failed_batches
```

### 4. æ—¥å¿—å’Œç›‘æ§

```python
# ä¸åŒçº§åˆ«çš„æ—¥å¿—è®°å½•

logger.debug(f"æ‰¹æ¬¡ {idx}: ä½¿ç”¨ OR è¯­æ³•æŸ¥è¯¢ {len(batch)} é¡¹")
# è¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼Œç”Ÿäº§ç¯å¢ƒä¸è¾“å‡º

logger.info(f"âœ“ æ‰¹æ¬¡ {idx}: è·å– {len(works)} ç¯‡è®ºæ–‡")
# å…³é”®ä¿¡æ¯ï¼Œç”¨äºè·Ÿè¸ªè¿›åº¦

logger.warning(f"âš ï¸  é€Ÿç‡é™åˆ¶ï¼Œ{wait_time}s åé‡è¯•")
# å‘Šè­¦ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¹²é¢„

logger.error(f"âœ— æ‰¹æ¬¡ {idx} å¤±è´¥: {error}")
# é”™è¯¯ï¼Œåº”è¯¥ç«‹å³æ£€æŸ¥
```

---

## ç›‘æ§å’Œæ—¥å¿—

### 1. æ€§èƒ½æŒ‡æ ‡

```python
import time
from collections import defaultdict

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.timings = defaultdict(list)
        self.error_counts = defaultdict(int)
        self.start_time = time.time()
    
    def record_timing(self, operation, elapsed):
        """è®°å½•æ“ä½œè€—æ—¶"""
        self.timings[operation].append(elapsed)
    
    def record_error(self, operation):
        """è®°å½•é”™è¯¯"""
        self.error_counts[operation] += 1
    
    def get_stats(self, operation):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        times = self.timings[operation]
        if not times:
            return None
        
        return {
            'min': min(times),
            'max': max(times),
            'avg': sum(times) / len(times),
            'count': len(times),
            'errors': self.error_counts[operation],
        }
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        elapsed = time.time() - self.start_time
        print(f"\n=== æ€§èƒ½æ‘˜è¦ (è€—æ—¶ {elapsed:.1f}s) ===")
        
        for op in self.timings:
            stats = self.get_stats(op)
            print(f"{op}:")
            print(f"  æ¬¡æ•°: {stats['count']}, å¹³å‡: {stats['avg']:.2f}s")
            print(f"  èŒƒå›´: {stats['min']:.2f}s - {stats['max']:.2f}s")
            if stats['errors'] > 0:
                print(f"  é”™è¯¯: {stats['errors']}")

# ä½¿ç”¨
monitor = PerformanceMonitor()

start = time.time()
response = make_request(...)
elapsed = time.time() - start
monitor.record_timing('make_request', elapsed)

monitor.print_summary()
```

### 2. æ—¥å¿—æ ¼å¼

```python
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# ç»“æ„åŒ–æ—¥å¿—
logger.info(f"âœ“ è·å–æˆåŠŸ: {len(works)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ {elapsed:.1f}s")
#            â†‘ emoji ä¾¿äºè¯†åˆ«
#                          â†‘ å…·ä½“æ•°å­—ä¾¿äºåˆ†æ

# æ—¥å¿—ç¤ºä¾‹
# 2024-11-16 10:30:45,123 - openalex_fetcher - INFO - âœ“ è·å–æˆåŠŸ: 1500 ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ 45.3s
# 2024-11-16 10:30:50,234 - openalex_fetcher - WARNING - âš ï¸ è§¦å‘é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… 2s åé‡è¯•
# 2024-11-16 10:30:52,456 - openalex_fetcher - INFO - âœ“ é‡è¯•æˆåŠŸ
```

### 3. å…³é”®ç›‘æ§ç‚¹

```python
def search_works(query, year_min, year_max, limit):
    logger.info(f"å¼€å§‹æœç´¢è®ºæ–‡: query={query}, å¹´ä»½={year_min}-{year_max}, limit={limit}")
    start = time.time()
    
    try:
        results = _execute_search(query, filter_str, limit)
        elapsed = time.time() - start
        
        logger.info(f"âœ“ è·å–æˆåŠŸ: {len(results)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶ {elapsed:.1f}s")
        return results
    
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"âœ— è·å–å¤±è´¥: {e}ï¼Œè€—æ—¶ {elapsed:.1f}s")
        raise
```

### 4. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
def benchmark_or_syntax():
    """OR è¯­æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    test_sizes = [10, 50, 100, 200, 500, 1000]
    
    for size in test_sizes:
        work_ids = [f'W{i}' for i in range(size)]
        
        # æµ‹è¯• OR è¯­æ³•æ‰¹é‡
        start = time.time()
        authors = get_authors_by_work_ids(work_ids)
        elapsed = time.time() - start
        
        print(f"è®ºæ–‡æ•°: {size:4d}, è€—æ—¶: {elapsed:6.2f}s, åå: {size/elapsed:6.1f} ç¯‡/ç§’")
    
    # è¾“å‡ºç¤ºä¾‹
    # è®ºæ–‡æ•°:   10, è€—æ—¶:   0.50s, åå:  20.0 ç¯‡/ç§’
    # è®ºæ–‡æ•°:   50, è€—æ—¶:   0.58s, åå:  86.2 ç¯‡/ç§’ âœ…
    # è®ºæ–‡æ•°:  100, è€—æ—¶:   1.05s, åå:  95.2 ç¯‡/ç§’ âœ…
    # è®ºæ–‡æ•°:  200, è€—æ—¶:   1.82s, åå: 109.9 ç¯‡/ç§’ âœ…
    # è®ºæ–‡æ•°:  500, è€—æ—¶:   4.35s, åå: 114.9 ç¯‡/ç§’ âœ…
    # è®ºæ–‡æ•°: 1000, è€—æ—¶:   8.67s, åå: 115.4 ç¯‡/ç§’ âœ…
```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ ¸å¿ƒä¼˜åŒ–æ€»ç»“

| ä¼˜åŒ– | æ•ˆæœ | éš¾åº¦ | å®ç° |
|------|------|------|------|
| OR è¯­æ³•æ‰¹é‡æŸ¥è¯¢ | 10-50 å€ | ç®€å• | âœ… å·²å®ç° |
| è‡ªé€‚åº”æ‰¹å¤„ç† | 10-20% | ç®€å• | âœ… å·²å®ç° |
| é€Ÿç‡é™åˆ¶æ§åˆ¶ | ç¨³å®šæ€§ | ä¸­ç­‰ | âœ… å·²å®ç° |
| ç¼“å­˜æœºåˆ¶ | 0-100% (å–å†³äºå‘½ä¸­ç‡) | ä¸­ç­‰ | âœ… å·²å®ç° |
| æ¸¸æ ‡åˆ†é¡µä¼˜åŒ– | 5-10% | ç®€å• | âœ… å·²å®ç° |
| å¹¶å‘è®¿é—® (ThreadPool) | 0% (ä¸é€‚ç”¨) | é«˜ | âŒ æœªå®ç° |

### æœ€ä½³å®è·µæ¸…å•

- âœ… å§‹ç»ˆä½¿ç”¨ OR è¯­æ³•æ‰¹é‡æŸ¥è¯¢å¤šä¸ª ID
- âœ… æ ¹æ®æ•°æ®é‡è‡ªé€‚åº”é€‰æ‹© batch_size
- âœ… ä½¿ç”¨ per_page=200 æœ€å¤§åŒ–æ¯é¡µæ•°æ®
- âœ… ä¸¥æ ¼éµå®ˆ 10 req/s é€Ÿç‡é™åˆ¶
- âœ… æ·»åŠ  mailto å‚æ•°åŠ å…¥ Polite Pool
- âœ… å®ç°æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
- âœ… ä½¿ç”¨å†…å­˜ç¼“å­˜ + Redis äºŒå±‚ç¼“å­˜
- âœ… è®°å½•è¯¦ç»†çš„æ€§èƒ½æ—¥å¿—
- âŒ ä¸è¦ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘ cursor åˆ†é¡µ
- âŒ ä¸è¦é€ä¸ªæŸ¥è¯¢è€Œä¸ä½¿ç”¨ OR è¯­æ³•
- âŒ ä¸è¦å¿½ç•¥é”™è¯¯å¤„ç†

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡

```
å°è§„æ¨¡ (<100 é¡¹):
- è®ºæ–‡æœç´¢: <1s
- ä½œè€…è·å–: <2s
- åˆä½œå…³ç³»: <5s

ä¸­è§„æ¨¡ (100-1000 é¡¹):
- è®ºæ–‡æœç´¢: 1-10s
- ä½œè€…è·å–: 5-30s
- åˆä½œå…³ç³»: 10-60s

å¤§è§„æ¨¡ (1000-10000 é¡¹):
- è®ºæ–‡æœç´¢: 10-60s
- ä½œè€…è·å–: 30-300s
- åˆä½œå…³ç³»: 60-600s (å— per_page=200 é™åˆ¶)

è¶…å¤§è§„æ¨¡ (10000+ é¡¹):
- è€ƒè™‘ä½¿ç”¨ Premium API æˆ– Group By èšåˆ
- æˆ–åˆ†å‰²ä»»åŠ¡ä¸ºè¾ƒå°æ‰¹æ¬¡
```

---

## å‚è€ƒèµ„æº

- [OpenAlex API æ–‡æ¡£](https://docs.openalex.org/)
- [é€Ÿç‡é™åˆ¶æŒ‡å—](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication)
- [OR è¯­æ³•æ–‡æ¡£](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists#addition-or)
- [æ¸¸æ ‡åˆ†é¡µæ–‡æ¡£](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging)
- [å®˜æ–¹æ€§èƒ½ä¼˜åŒ–åšå®¢](https://blog.ourresearch.org/fetch-multiple-dois-in-one-openalex-api-request/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2024-11-16  
**ç»´æŠ¤è€…**: UFCT Backend Team
